\chapter{Conclusions}
\label{sec:conclusions}
This manuscript marks the culmination of 40 months of my doctoral journey, with a central focus on developing data structures to represent sets of human genomes as a pangenome—primarily through the use and development of \kmer based tools. Every step of this work has been driven by the goal of creating software or analyses that can address relevant biological questions.

The path, however, was anything but linear. Along the way, I got involved in a variety of projects, some of which aren’t directly proposed in this manuscript but nonetheless provided valuable lessons. These experiences helped deepen my understanding of bioinformatics, computer science, and genomics, which is why the scope of this thesis is relatively broad. It starts by analyzing computational pangenomics methods for representing human genomes and ends with the development of efficient data structures for handling \kmers. In my view this is the result of constant curiosity about sequence bioinformatics problems, a collaborative spirit and the reflection of a comprehensive and multifaceted approach to the challenges of the field.

One key contribution of this work has been an analysis of pangenome graphs constructed from high-quality human haploid assemblies, which was well received by the research community. The findings highlighted the strengths and caveats of both how these models are represented and the methods used to generate them. Specifically, we demonstrated that different types of graphs serve different purposes: variation graphs are intuitive for downstream analysis and easier to manipulate, whereas de Bruijn graphs are more scalable and efficient at preserving the input sequences, offering better performance for large datasets. However, as with many areas in genomics, there is no "one-size-fits-all" solution. This research also pointed toward areas where \kmer based approaches could become more useful for genome-wide analyses, something that still requires attention and development. Overall, I believe pangenomics holds the key to addressing many of the limitations of current genomics approaches, particularly as it improves to help us better understand structural variations in primate genomes.

In my work on \kmer based data structures, I explored several different methods and implementations, often revealing the complexity of finding models that can address multiple problems simultaneously. A significant challenge in pangenomics (and in bioinformatics broadly) is finding the right trade-offs for specific applications, and this work spans multiple levels of development—from high-level script design to low-level operational optimizations for more efficient tools.

My work on quotient filters has centered on re-developing the Rank Select Quotient filter, aiming to make the underlying data structure flexible enough for a variety of high-level applications. This led to successful implementations such as the application of the Fimpera scheme on top of a traditional CQF, and the creation of a new data structure, the BQF.

my research on super\kmers resulted in a prototype to evaluate sorted super\kmer lists in practical, real-world applications. Although it is not at the level of a standalone tool for a specific application, it can be used as a building block of a more comprehensive \kmer based tool-set for pangenome sequence manipulation and analysis.

I also contributed to other exciting projects, such as \muset, a pipeline for creating uniting matrices (both in terms of abundance and presence/absence). This work represents an important step forward from traditional \kmer matrices, simplifying some of the complexity in downstream genomic analysis while retaining necessary information. Collaborating on this project has been important, as I believe the field of \kmer based tools needs more downstream tools for specific genomic applications.

Though the tools, pipelines, and data structures presented here are not the final solutions to every challenge in pangenomics, I hope they represent meaningful steps in the right direction. Undoubtedly, there’s still much work to be done. The promising areas that the content of this manuscript can help address are scaling pangenome representations for large collections of eukaryotes, establishing methodologies to validate the biological accuracy of pangenome graphs, similar to how we validate genome assemblies today and developing more \kmer-based tools to help the genomics and genetics community in their analyses.

\begin{comment}

This manuscript is the result of 40 months of doctoral journey. The main thread is the use and development of data structures to represent a set of human genomes as a pangenome, with a special focus on \kmer based tools. This effort has been always driven by the desire to produce software or analyses that would help answer biological questions.

It hasn’t been a straightforward path as I embarked into many different projects some of which are not reported in this manuscript. Nevertheless all efforts taught me important lessons about doing research in the bioinformatics field and strengthened my understanding of computer science and genomics. This is reflected in the quite broad scope of this doctoral thesis, that start with an analysis of computational pangenomics methods for human genomes representations and ends with data structures to represent \kmers in a an efficient way. In my view this is the result of constant curiosity about sequence bioinformatics problems tackled by the team that are linked to pangenomics and the reflection of a comprehensive and multifaceted approach to the challenges of the field.

We proposed an analysis on the construction and representation of pangenome graph from high quality human haploid assemblies that was well received by the community as it shed light on the characteristics of both the internal representation and the methods to generate these models. It stresses the importance of selecting the kind of graph that best fits the particular application, specifically in the way it represents variations in the DNA sequence of the individuals. Variation graphs are better to perform specific downstream analysis and are more intuitive to understand, manipulate, visualize and analyze while de Bruijn Graphs are more efficient to generate, scale better and give guarantees on the preservation of the input sequences. Finally, this work stresses the difficulties of proposing a one-fits-all solution and points out areas of research that would make \kmer based approaches more attractive to the genomic community. I believe pangenomics is the key of solving many issues and deficiencies present in current genomics approaches: it is a novel area that is growing and evolving now and will need more effort to produce solutions that will help understand the structural variations in primates genomes.

My work on \kmer based data structures that can be used for pangenomics applications has spanned on different approaches, methods and implementations. This stresses the difficulty to find models that can solve multiple problems at the same time and the need to find an acceptable trade-off for any specific application. This work spaced at different levels of computer science development, from high level design of scripts to low level operations to develop tools that can be efficient.

My work on quotient filters has been focused on coding the underlying data structure, the Rank Select Quotient filter, in a way that could be easily used and manipulated for different high level interfaces that would implement different filters. This enabled us to use the Fimpera scheme on top of a normal CQF as well as the development of a new data structure, the BQF.

My work on super\kmer has produced a prototype to evaluate the performances of sorted super\kmer lists for real-life applications. Even if the result is not a standalone tool to use in specific applications, I believe is a useful contribution to the community.

Finally, I gave my contribution to other projects, like \muset, a pipeline for the construction of uniting matrices (both abundance and presence/absence). This is the first effort to produce such data structure and is an advancement from \kmer matrices, that contained almost the same information while being more complex to analyze. I am happy to have collaborated on this project as the \kmer field needs new tools to perform downstream analysis of genomes.

The tools, pipelines and data structures I analyzed or coded are still not the definitive answer to any of the problems that pangenomic still face but are a clear demonstration of what are good steps in the right direction. In the future there is plenty of work to do. In the pangenomic field, methods to demonstrate the biological correctness of pangenome graphs, like it is now done with assemblies, as well to scale pangenomic representation to handle large collections of eukaryotes.
\end{comment}

%This data structure can serve different applications, from pangenomics to metagenomics data as well as cancer transcriptomics, as it provides a very clear representation of the genetic content of multiple samples and their difference that is easily accessible by data analysis and machine learning tools.\\

%The final method proposed on top of this data structure a particular encoding of the counting information together with the integration of the Fimpera scheme for \kmer storing and retrieving. My work lied on the demonstration of the efficacy of the implementation I wrote by recreating on top of it the Counting Quotient Filter as originally conceived together with the Fimpera scheme. This demonstrated that implementation differences affect the magnitude of the data such data structures can analyze, even if the information provided as output is the same. This work served as confirmation that  improvements in the representation ok \kmer sets can drastically change the way in which large data collections can be interrogated and explored, even if very complex. In order to improve in the future such analysis power, new tools will require enhanced methods and refined coding techniques to exploit the maximum out of computational power to analyze exponentially increasing amounts of data.\\

%In metagenomics new methods will need to follow the pace of the rapid increase of data that is being sequence everywhere in the world.\\
%Finally, all the methods that will need to analyze and compare multiple individuals or samples will need to use \kmers as part or as bedrock of their development, as I think this thesis proved, are a simple, versatile and effective way of representing DNA sequences.

%As the understanding of public metagenomic repositories is important when developing software that is meant to encode their informations, I also helped in the writing of a section about such repositories in a method primer review. My contribution to this large collaborative effort was to provide to potential users a first, concise and expert overview on which are the main efforts to store, categorise and analyse public metagenomics data.\\ 
