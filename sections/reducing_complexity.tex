\chapter{Exploring new \kmer based methods for Pangenomics} %Reducing complexity of 
\label{sec:complexity}

\section{Introduction: using \kmer sets in pangenomics}%too big, too complex, too difficult to use
As discussed in the previous sections of this manuscript, the construction of pangenome as variation graphs is based on an alignment step that is known to be accurate but computationally expensive. Even if recent advances on alignment algorithms and tools, as the wavefront algorithm~\cite{wavefront} or full-text indexes as the r-index~\cite{spumoni2} and move index~\cite{movi} have provided improvement in construction time or query performance.\\
The variation graph represents an effective methodology for conducting thorough analyses of a curated set of samples. As an illustrative example, at the time of this manuscript's composition, the Human Pangenome Reference Consortium is in the process of releasing an additional cohort comprising approximately 220 high-fidelity human genome sequences. These genomes are intended for integration into the development of an updated reference pangenome for the human species.\\
The alignment step implicitly requires high-quality complete assemblies to produce reasonably connected graphs. While it is anticipated that the availability of such high-quality genomes will continue to grow in the coming years, there is currently a large quantity of raw (or minimally processed) data that can be utilized in pangenomics applications~\cite{serratus,logan} but cannot be harnessed by variation graph models.\\ 
For this reason, \kmer-based approaches provide a robust alternative. As the \kmer length used is typically relatively small (from 21 to 100), these approaches can be applied to more fragmented assemblies or directly to raw sequencing reads, and their scalability has been proven to be orders of magnitude superior to variation graphs. Moreover, they can be used to build representations from data of varying quality, such as phased assemblies from one cohort and unitigs from another. These tools usually employ different data structures to represent a de Bruijn graph model internally and efficiently. The main challenge of these data structures is primarily the amount of space used to represent the \kmers versus the time used to query elements (single \kmers or sequences). Consequently, implementation decisions are often bound to optimization compromises made to achieve specific goals: disk compression to produce small-sized indexes from large collections, fast query time of a given sequence, or time/memory trade-offs.\\
The computational resources necessary for generating compacted colored de Bruijn graphs from a set of input genomes are, as shown in the preceding chapter, reduced in comparison to those required for variation graph construction. Furthermore, the tools employed in this approach demonstrate superior scalability, allowing for the processing of substantially larger genomic collections.\\
As \kmer-based methods present valid alternatives for pangenomic studies, I focused part of my PhD on studying and developing data structures that could find useful applications in pangenomics. Here, I will present three projects that yielded relevant outcomes. In two of these projects, I had the opportunity to collaborate with other researchers, both within my unit and from other groups. For these projects, my work was included as a component in a larger framework. While I cannot claim to have carried each of these projects as a whole, I nevertheless brought significant input to each of them.

The chapter is organized as follows:
\begin{itemize}
	\item Introduction on \kmer sets and metadata representation: why it is needed and how;
	\item Overview of our contributions and my part on them;
	\item \muset: from graph to matrices for downstream analysis;
	\item Prototyping dynamic data structures for \kmer counting:
	\begin{itemize}
		\item Re-implementation of a Quotient Filter as a base for multiple applications;
		\item Explore dynamicity without indexing: \skmer sorting;
	\end{itemize}
	\item Summary and conclusions.
\end{itemize}
%one of which is a step into providing a different representation of pangenomes as \kmer sets more suitable for downstream representation.

\section{Introduction: sets of \kmers and metadata association}
\kmer-based data structures  for representing sets of input genomes inevitably involve trade-offs to support useful applications in pangenomics.  The key characteristics of such structures are often in competition with one another, requiring a balance between the following:
\begin{itemize}
	\item Efficient storage of the data;
	\item Fast querying of a \kmer or string to retrieve its associated metadata;
\end{itemize}
The process of optimizing data storage for quick lookups is referred to as indexing, whereas the processes of retrieving the metadata and determining membership are called querying. Ideally, these data structures should be capable of performing both tasks efficiently, especially as datasets grow in size—as discussed in the introduction of this chapter. Given that genomic data is expanding at near-exponential rates, minimizing both storage requirements and query times for \kmer sets has become increasingly crucial.\\
A simple but very effective analogy of indexing and querying can be done with books and words.
Imagine I recall having a book in my library featuring a protagonist named Ricardo whose narrative unfolds also in Paris. Without a systematic organization of my collection, I might need to sequentially examine each book's content from beginning to end. This is an incredibly inefficient process, especially in the case of a large library. A more effective approach for me would be to maintain a comprehensive index of all words contained within my library's volumes. This system would allow me to rapidly identify books containing specific terms, such as "Ricardo" or its diminutive form, "Ricardito". Furthermore, if I were to implement an additional index associating geographical locations with books featuring scenes set in those places, I could cross-reference the occurrence of the name "Ricardo" with Paris as a location and, without even needing to open a single book, I would find that the book I'm recalling is \emph{Travesuras de la niña mala} of the Nobel Laureate Mario Vargas Llosa.\\
This is an example of how, indexing sequencing data, \emph{the words} and their metadata, \emph{the places}, one can rapidly check which samples, \emph{the books}, contain a requested value, without having to look at the raw data (the content of the book).\\
As the \dbg is a model for a \kmer set representation, under the hood there are different data structures that can be used to store the \kmers and index them for efficient retrieval. These data structures can be divided into exact and inexact data structures. In the rest of this section I will present the characteristics of such data structures and briefly mention some propaedeutical to the prototypes we developed.

\subsection{Hashing \kmers}
As described in section~\ref{sec:kmer}, a \kmer is a sub-string of length $k$ of a biological sequence. In order to reduce the space used to store them, the text string is converted, or hashed, into a binary string that can therefore be interpreted as an integer number. The use of the equivalence of a binary string with an integer is the basis of a great part of \kmer-based data structures: from here on I will consider methods for \kmers representation as binary string using hash functions. Methods that consider \kmer as text string won't be therefore considered.
\begin{description}
	\item An hash function is any function that maps data from one set (usually text but not only) to another (usually fixed-size machine-word-length integers). The ingested value is called key and the output is usually called hash value or simply hash. They are used in a lot of applications such as basic computer science models such as dictionaries, cryptography or bioinformatics.
	\item Hash functions should optimize on some of these properties:
	\begin{itemize}
		\item[\textbf{Uniformity}] In most cases input data should be mapped in an uniform way in the output space: in the \kmer case, lexicographically similar \kmers should be mapped to different hashes. This is not true when data locality matters: the property of similar \kmers pointing to similar hashes can be used by some data structures to avoid many cache misses, improving the computation time.
		\item[\textbf{Speed}] the fastest it is possible to compute a hash from a key, the better it is. Speed depends on the number and latency of the operations executed in the computation;
		\item[\textbf{collision avoidance}] collisions, i.e. mapping different keys into the same hash, should be infrequent. The collision rate is correlated to the size of the hash space and therefore to the space that can be used to store the hash. This trade-off will be explored better in the next section.
	\end{itemize}
\end{description}
Moreover, \kmer length impacts the time/space trade-off stated in the previous section: as larger \kmer offer greater specificity, they largely increase the amount of space needed to store them (because the hash will probably be larger) as also shown for plain-text representation in section~\ref{sec:kmer}.

\subsection{Minimum set of operations and metadata}
Given a set of sequencing samples, the data structure should support the addition of \kmers from each sample through an \emph{insert()}  operation during its construction. In some data structures, the order of the input data doesn't matter, while others require a specific order for insertion to function correctly. As we'll discuss in more detail in Section \ref{sec:staticdynamic}, post-construction insertion isn't always a guaranteed feature in certain implementations.\\
Normally these data structures can return two kind of information: the presence absence of an element, using a \memb operation and metadata associated with the specific \kmer. There are some implementations in which:
\begin{itemize}
	\item only the presence or absence can be retrieved, using the \memb operation. Bloom-filters, that will be presented in section~\ref{sec:bloom_filters}, fall under this category.
	\item only metadata can be retrieved. Minimal Perfect Hash Functions (MPHF), that won't be discussed in this manuscript, are maps of a static set of known keys to metadata that do not support \memb operations~\cite{mphf}.
	\item both presence and metadata can be queried, as in color compacted \dbg, presented in section~\ref{sec:ccdbg}.
\end{itemize}
Metadata is a broad keyword that I will use to identify information associated to the \kmers represented in the data structure itself. Presence or absence of a \kmer in the set is usually, with some exception such as \gls{MPHF}, directly encoded in the insertion of the elements in the data structure and do not require additional bits. Data structures that report only absence or presence are considered to not support metadata. The ones that do support different kind of metadata are considered associative, i.e. associate metadata to the \kmers.\\
A data structure may or may not retain internal information after executing a membership query. For instance, when searching for an element in a dictionary, the CPU ends up accessing a chunk of memory that includes the queried key-value pair along with other closely located data. Some data structures explicitly store variables to remember exactly where in their internal representation the most recent \memb operation occurred. This becomes especially important when sequences, not individual \kmers, are queried, as sequential \memb operations are performed on \kmers that typically overlap. Efficiently leveraging these properties can significantly impact the scalability and performance of these data structures.

\subsubsection{Metadata types}
In the most fundamental case, the data structure represents the presence or absence of an element within it, utilizing a binary \memb operation (denoted as 0 for absence and 1 for presence). Other metadata that can be useful in pangenomics can store:
\begin{itemize}
	\item[\textbf{counts}] The data structure contains the number of times a \kmer appears in it. Counts are useful to discern copy variants number in different DNA samples, the expression of genes in RNA samples and for species abundance estimation in metagenomic samples.
	\item[\textbf{colors}] It stores in which samples a \kmer has been seen and returns a list of containing samples for each queried \kmer.
	\item[\textbf{Id}] In applications in which the graph structure is relevant (for example in visualization), it is useful to know in which \kmers (in the case of \dbg) or unitigs (in the case of \cdbg) of the graph are represented the \kmers of a queried sequence. This case is relevant for \dbg based models.
	\item[\textbf{Text}] Text data can be used to associate \kmers to genetic information as genes, regulatory elements, flags to discern pathogenic variants from non-pathogenic ones and so on.
\end{itemize}
Of these metadata, the first two are the ones that are usually taken in consideration for query by recently developed data structures. Text data would impose a significant space requirement for the data structure and could be mimicked by assigning numerical labels to text. Just in the case of text metadata query an additional map associating numbers to text would be used. The id information is quite overlooked by, to my knowledge, all implementations.\\
We will now turn out attention to sets of \kmer sets, where each input sample is considered as a different set. This is done by using colors, that make possible to retrieve from the data structure in which sample a \kmer can be found. The color terminology can be confusing as it is often used to refer to two different concepts. From here on, the term color is used to refer to the data set from which a set of \kmer comes from. A \kmer $x$ will have colors 1,2,3 if it comes from dataset 1,2,3. Sometimes \kmers are associated to color sets, that represent a specific group of colors. For example the color set A will contain colors 1,2,3 so in the data structure \kmer $x$ is associated to the identifier A and the dataset of origins can be retrieved by remembering the map from color-set to colors. The use of color sets instead of directly associate a \kmer with its colors allows efficient storage\cite{ggcat}. 

\subsubsection{Metadata: why it is important}
Metadata is important to enable different kind of applications that need more information than just the presence or absence of a \kmer in a set. \\
In some applications, it's important to understand how many times a particular genetic sequence is repeated, and \kmer counting can serve as a useful proxy for that. For instance, the abundance of specific RNA molecules in a cell can help differentiate between normal and cancerous activity~\cite{kamrat,transipedia}. Similarly, variations in the copy number of specific \gls{DNA} regions can distinguish between phenotypes, revealing important differences between samples in a pangenome. In \dbg models, counting \kmers is essential to capture the multiplicity of repeated sequences, whereas in variation graphs, copy number variants are implicitly represented within the paths themselves.\\
In some applications it is important to discern between the different samples used to fill the data structure, hence representing sets of \kmer sets~\cite{metadbg}. Colors are vastly used in pangenomics, as they allow to keep track of the genomes associated to variations and the ones that are part of the core genome, both in bacteria and in eukaryotes.\\
Remembering the \dbg overlap structure is also important in many applications that rely on visualization. This would enable fast subgraph identification for loci of interest and enable specific genomic applications for \dbg based methods. For example \ssh, an indexing data structure for unitigs, would be suited for this scope~\cite{sshash}.\\
Finally, part or all of these metadata might be useful to be stored at the same time for many applications, including pangenomics. For example, \kmer counts and colors are useful to differentiate copy number repeats between different genomes in a population. Counts and genome positions can be used  for lossless reconstruction genomes.
%The result of the \memb operation can be exact or approximate, i.e. have false positives. 

\subsection{Basic data structures: sorted list and hash table}
The most basic data structure used in computer science to maintain an ordered collection of elements for efficient searching is a sorted list. By ordering the entire enumeration of the set of \kmers in each sample, one can use a binary search algorithm to find a requested \kmer in time $O$($k$log$n$), using $O$($kn$) space, with $n$ the cardinality of the set. The use of sorted lists for \kmer storage and retrieval is feasible for very basic cases involving small sets of \kmers. However, this approach becomes intractable for the large-scale genomic applications previously mentioned, due to poor scaling in both query time and storage requirements. Nevertheless, sorted list can be used in case the number of elements is greatly reduced (by using compacted \kmer representations for example) and to avoid costly indexing. More on sorted lists is going to be discussed in section~\ref{sec:skmers}.\\
Hash-tables, a well known implementation of dictionaries (or maps) in computer science, associate keys and values using hash functions. Hash tables solve the problem of the query time, bringing it to $O$($k$) or $O$(1), depending on the particular hash function used. As they still require $O(kn)$ space, general hash table implementations can be used when dynamicity is required and the space (bit per \kmer) is not a major bottleneck: Rust HashSet implementation has shown to be useful in particular genomic use-cases, as to find in which reads are present \kmers belonging to a specific set. I provided a small contribution on this particular subject by helping with the development of Back to Sequences, a tool written in Rust that uses an hash table to find in a set of samples $S$ the reads $r$ containing \kmers present in a set $K$. The tool outputs the sequences containing the \kmers, using both a minimum and maximum threshold given as input~\cite{back_to_sequences}.\\

\subsection{Approximate membership and filters}
Approximate Membership Query (\gls{AMQ}) data structures offer a trade-off between storage space (in memory or on disk) and query accuracy to enhance space efficiency. Unlike sorted lists or hash tables that always return correct information, AMQ structures respond with a non-zero probability of false positives (i.e., reporting the presence of an element that is not actually in the dataset). Due to this probabilistic nature, they are often referred to as probabilistic data structures.\\
Filters are a subset of AMQs frequently used in genomic applications. They allow fast answers on negative queries. Their basic value can be either a single bit (hence bitvectors) or any amount of bits that ensure optimal space-efficiency. These 'slots' that contain data can be smaller than a machine word or a single byte, using low-level implementation operations.

\subsubsection{Bloom Filters}
\label{sec:bloom_filters}
Bloom filters represent the most widely utilized probabilistic data structures, finding applications in diverse fields. In genomics, they are employed for tasks such as the elimination of contaminant-related reads from ancient DNA samples~\cite{akmerbroom}. Beyond genomics, their utility extends to various computational domains. For instance, they are used to reduce disk lookups for non-existent rows or columns in databases, and they are part of Google Chrome's protection mechanism against malicious URLs~\cite{dbbf,chromebf}. In genomics they are used to provide a very space-efficient representation of a set of \kmers by using a bitvector and multiple different hash functions. When a \kmer is inserted, multiple different hashes are generated and the position in the bitvector corresponding to the hashes are set to 1. When an element is queried, the same hash functions are applied and if all positions in the bitvector are set to 1 the element is considered present. If at least one position is set to 0 it means that the element is not present, thus preventing false negatives. As collision can happen, especially when using multiple hash functions, it is instead possible that a position associated to the output of a hash function of a \kmer was set to 1 by the output of another hash of another \kmer, leading to false positives, i.e. reporting a \kmer is inside the data structure while it is not present.\\
Counting bloom filters store counts instead of presence/absence in the vector positions. The value they return depends in the way elements are inserted. A \kmer count gets updated if a value larger than the stored one gets inserted. When a sequence is queried, they return the minimum value of the counts associated to the composing \kmers.\\
Interleaved Bloom filters are instead made of several Bloom filters, as each one is filled with \kmers from a specific sample. The filters are divided into chunks and each specific chunk of all filters can be found in a contiguous region of memory. This speeds up color queries, as the lookup is done in a specific constrained region, limiting cache loading or misses.\\
Multiple implementation and optimization techniques, such as the blocked-Bloom filters used to speed query and insert operations, are used to maximize the potential of this data structure won't be addressed here but are thoroughly explained in these reviews~\cite{marchet2024kmersets,marchet2021kmer,marchet2024coloredkmersets}. 

\subsubsection{Quotient Filters}
\label{sec:introduction_qf}
Quotient filters are another data structure that is based on the idea of filling a vector with metadata but it does so in a different way compared to the Bloom filter.
The hash computed from the \kmer get separated into two parts: the quotient (leftmost or rightmost bits, depending on the implementation) and the remainder (the rest). The size of the quotient depends on the amount of data that is being stored. Instead of filling the vector with the metadata at the position associated to the whole hash, it fills the slot at the position associated to the quotient with the bits of the remainder. In order to avoid collision when hashes with the same quotient occur, the remainders of a quotient are stored in order in successive slots, also called runs, to preserve the information and enable fast queries. This is done by using companion data structures that are used to trace where the run of a quotient is in the vector. When metadata such as counts has to be stored, multiple slots can be used to encode the count of a single remainder. This is done in the Counting Quotient Filter. Another possibility is to reserve some bits of the slots to store the count, as in the Backpack Quotient Filter.
These filters enable collision resolution by using slots in a more flexible way. More about this data structure will be discussed in section~\ref{sec:qf}.

\subsection{Static vs Dynamic data structures}
\label{sec:staticdynamic}
Another important feature of \kmer data structures is whether they can be modified after construction, which brings us to the distinction between static and dynamic data structures.\\
\begin{itemize}
	\item \textbf{Static data structures} cannot be altered once they have been built.  Addition or removal of elements from the original set is not supported. To modify the underlying set, the entire data structure has to be recreated. Their advantage is efficiency, as they usually allow for greater compression and use less space making them ideal for applications where a stable reference set is used, and there isn’t a need for frequent modifications. For example, they’re well-suited for comparing new datasets against a reference genome that remains constant.
	\item \textbf{Dynamic data structures} allow updates after their initial construction. The types of modifications that can be made depend on the specific implementation and the use case. The most common operation is the insertion of new \kmers, which effectively functions as an union of two sets (for presence or colors). In the case of counting structures, usually a count of an existing \kmer is updated if last inserted has a greater count. Other possible updates include deleting \kmers or modifying the metadata associated with them.
\end{itemize}
While most methods are static, dynamic structures that allow efficient insertion and, less frequently, deletion of \kmers are being developed in recent years~\cite{marchet2024kmersets}.

\section{Our contributions: an outline}
The three projects span different topics and can be divided at 2 different levels of engineering. The first is mainly organizing a pipeline with some already developed bioinformatics tools and contributing to the development of a tool for \kmer information manipulation. The other two are development of a tool from scratch.\\
They can be presented as follow:
\begin{itemize}
	\item[\textbf{\muset}] is a pipeline to construct plain text unitig matrices from input sample. It enables to build an abundance matrix in which the unitig count in each sample is the average of the counts of its constituent \kmers and a presence/absence matrix that report an unitig as present in a sample if its constituent \kmers are present in the sample over a given threshold.\\
	\item[\smash{\stackunder{A \textbf{Quotient}}{\textbf{filter}}}] implementation that allows dynamic updates, resizing, and a framework to develop different specific data structures on top of it. It is the building block of a novel data structure, the Backpack Quotient Filter that has been recently published. I also redeveloped a Counting Quotient filter that uses a special scheme that uses less space by mimicking large \kmers by store smaller ones.\\ 
	\item[\smash{\stackunder{A \textbf{\skmer}}{\textbf{sorting}}}] implementation to explore a different data structure to store \kmers without indexing and allowing queries.
\end{itemize}
Some of the research and development I did, mostly in the second and third projects, can be labeled as exploration and prototyping as the result is not intended to be a novel tool to be widely adopted by the community but as first step in possible route of research in this domain.

\section{\muset: building unitig matrices for downstream analyses}
In this section I will present the work that I have been doing on building unitig abundance matrices.
\subsection{Rationale}
Recent advancements in genomic sequencing technologies have led to the generation of massive datasets from large-scale projects. Some of them very functional to human pangenomics, such as the 1K Genomes Project and the HPRC, while others focus on related genomic areas like transcriptomics (e.g., GEUVADIS~\cite{geuvadis}) and metagenomics (e.g.,  MetaSub~\cite{metasub}, Tara Ocean~\cite{tara}).\\
In pangenomics, these large datasets present significant challenges for traditional variation graph analyses due to their size and complexity. \kmer-based methods offer an alternative approach to study this data, using techniques such as \kmer counting and matrix representation. These methods can lead to improved accuracy in estimating the abundance of loci across multiple samples, enabling more comprehensive analyses of complex genomic datasets. A potential application of these methods could allow \gls{GWAS} studies to be conducted on all possible variations within genomes, expanding beyond the current focus on \gls{SNP}s and small indels. Additionally, \kmer abundance matrices could serve as training data for Deep Learning models to, potentially uncovering traits that distinguish healthy from non-healthy populations for specific diseases.\\
Given the usefulness of these applications, we propose a novel method for constructing plain text abundance unitig matrices. These matrices can be directly utilized in various downstream applications, offering a valuable resource for researchers in genomics and related fields.

\subsection{Related work}
Cutting-edge tools that compute a \cdbg or \ccdbgs from input samples have been developed in recent years. While \bcalm and \cuttlefish generate \cdbg (hence a \kmer set) that do not record the sample of origin, \bifrost and \ggcat do build \ccdbgs that use colors to trace the source of the \kmers. While \ccdbgs are an implicit representation of an unitig matrix, as they contain the same information (unitigs and origin of \kmers) but represented in a different way, tools that build them do not produce a matrix as output nor provide any APIs or scripts to do so.\\
Recently, \kmt~\cite{kmtricks}, a very fast tool to build a \kmer abundance matrix from a set of samples has been proposed but the cardinality of the \kmer set obtained from input data renders these matrices poorly tractable for the aforementioned downstream applications. This is not a limitation of the tool but a feature of the \kmer spectrum of the datasets.\\
Remembering that unitigs, as described in section~\ref{sec:kmerobjects}, are a more succinct representation for \kmers, we propose a pipeline that mix the strength of both \cdbg tools to build unitigs and \kmt to represent \kmer color and abundance to produce unitig matrices that are more tractable for analysis. We also propose a simple pipeline to build presence absence unitig matrices from samples using a script that renders in a digestible text format the implicit representation of a \ccdbg.
\subsection{From sequencing data to unitig matrices}
\subsubsection{Main features and differences of unitig matrices}
The construction of an abundance unitig matrix is based on two key observations: first, \kmer matrices can now be constructed with high efficiency, and second, unitigs can also be built efficiently. Therefore we can create a more compact and manageable representation that preserves the high-level information crucial for genomic variation diversity studies, loci variation visualization, and input for machine learning libraries.\\
The main parts of the process to generate an abundance unitig matrix from an abundance \kmer matrix involves compacting \kmers into unitigs and estimating unitig abundance by averaging \kmer counts. In this way we obtain a more condensed, lossy representation of the data. As single \kmer counts are lost, the essential information is preserved by the average count of the unitig. Given that \kmer counts typically exhibit minimal variation within a unitig, this representation achieves compaction into a computationally manageable form, albeit at the expense of a minor loss of information.\\
In cases where abundance information is not required, presence-absence unitig matrices can be used instead. They still capture sequence variation between individuals and are valuable for diversity studies.\\
A key difference between the proposed model for abundance or presence-absence matrix construction process is a filtering step that retains only \kmers reflecting differences between samples. This targeted approach focuses on the most informative genomic regions and is applied on the abundance matrix construction. The presence-absence matrix includes the entire set of \kmers from the input genomes, providing a comprehensive view of the input data.\\

\subsubsection{Matrix construction overview}
Formally, given an unitig $u$ searched in a sample $S$, the \kmer presence ratio is defined as follow:

\begin{equation}
	f(u, S) = \frac{\sum_{i=1}^{N}{x_i}}{N}
\end{equation}

while the average abundance of a unitig $u$ with respect to a sample $S$ is defined as:

\begin{equation}
	A(u, S) = \frac{\sum_{i=1}^{N}{c_i}}{N}
\end{equation}
In the equations $N$ is the number of \kmers in $u$, and $x_i$ is a binary variable that is $1$ when the $i$-th \kmer is present in sample $S$ and $0$ otherwise, while $c_i$ is a non-negative integer count.\\
Figure~\ref{fig:muset} shows the main steps of the \muset pipeline. To produce an abundance matrix, the main steps are:
\begin{enumerate}
	\item A \kmer abundance matrix is built from FASTA/FASTQ files using \kmt;
	\item \kmers that are present in at least 10\% of the samples and absent in at least 10\% of them are retained, while the others are discarded. The thresholds are customizable;
	\item Unitigs are created from this set of retained \kmers in order to compress the representation. Unitigs shorter than a certain value are discarded. While this variable can be modified by the user, our recommendation is to keep it as $2k-1$ with $k$ the length of the \kmer. This value is the minimum value to observe a SNP in the set as an unitig representing a SNP would have 2 times $k-1$ bases as overlap to the unitigs representing the adjacent bases in the genome and 1 base for the variation.
	\item The abundance unitig matrix is therefore constructed. This is done by 
	\begin{enumerate}
		\item creating a dictionary, using \ssh, to link \kmers to the unitig in which they have been compacted;
		\item each unitig abundance score is computed by summing the count of its constituent \kmer set divided by the cardinality of the set. This is done independently for each sample to retain the color information of the \kmer matrix.
	\end{enumerate} 
\end{enumerate}
To generate a presence-absence the main steps of the pipeline are:
\begin{enumerate}
	\item unitig matrix the \ccdbg is built using \ggcat. Unitigs are in FASTA format while colors are stored in a compress representation accessible only via \ggcat \gls{cli} or \gls{API}s.
	\item unitigs are filtered by length like in step 3 of the abundance pipeline;
	\item filtered unitigs are queried against the \ggcat color index and for each sample in which at least 1 \kmer of the unitig was present, the presence ratio is reported. If no \kmer was present in the sample, the sample is not reported.
	\item the unitig query is then parsed (from jsonl) and the presence (1) or absence (0) is reported for every unitig in every sample in form of a matrix. The presence is determined when the fraction of present \kmers in the sample is above a pre-defined, although modifiable, threshold. It is also possible to produce a matrix that does report the presence ratio instead of a binary value.
\end{enumerate}
Only the abundance pipeline has been tested against the most similar state-of-the art tool that is in fact \ggcat, that produces, as mentioned, an implicit presence/absence matrix. Even without the \muset script to produce an explicit one, the abundance matrix script is faster than \ggcat when run on a large collection of 360 ancient oral samples, as shown in table~\ref{tab:muset_comparison}.\\
It's important to note the differences in output formats and functionality between our tool and \ggcat. \ggcat utilizes a JSONL (JSON Lines) format, where each line represents the result of a sequence query. In its color query mode, it only outputs colors (representing samples) with more than one \kmer present in the sequence, providing the ratio between present \kmers and total \kmers in the sequence. While this output can serve as a useful input for developing new applications, it is not sufficiently comprehensive on its own for many analysis applications. Recognizing this limitation, we developed the additional \muset script to transform \ggcat's output and produce a practical and widely applicable unitig presence/absence matrix, as described in the steps just outlined. This matrix provides a clear, binary representation of unitig occurrences across all samples, which is crucial for various downstream analyses in comparative genomics and pangenomics. The \kmat function that transforms the JSONL into a CSV with or the presence ratio for all samples or a binary 1/0 - presence/absence value (using a given threshold), is also useful for other applications that use \ggcat.
No computational resources test has been done on human genomes as it was out of the scope of the pure demonstration of the usability and efficiency of the method.
\begin{table}[!t]
	\centering
	%\tabcolsep=2Spt
	\begin{tabular}{lccc}
		\toprule
		Method & Wall-clock time & Peak memory & Disk usage \\
		\midrule
		\muset & \textbf{9h 43m 12s} & \textbf{\SI[detect-weight=true]{19}{\textbf{\giga\byte}}} & \textbf{\SI[detect-weight=true]{1.5}{\textbf{\tera\byte}}}\\
		\ggcat & 24h 20m 40s & \SI{167}{\giga\byte} & \SI{641}{\giga\byte} \\
		\bottomrule
	\end{tabular}
	\caption{Comparison of running time, peak memory, and disk usage between \muset (filtered unitig matrix) and \ggcat (implicit and unfiltered unitigs) on 360 ancient oral samples.}\label{tab:muset_comparison}
\end{table}

\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{figures/kmer_methods/muset_full.pdf}
	\caption[The \muset pipeline]{Scheme representing the main steps of the \muset pipeline.}
	\label{fig:muset}
\end{figure}

\subsection{Conclusions and perspectives}
\subsubsection*{Unitig matrices are pangenomes}
Matrices, although often overlooked in the pangenome community, offer a valid and valuable representation of pangenomes. Each genome can be conceptualized as a binary vector within a matrix that encodes the alleles of a pangenome graph. Both variation graphs and \ccdbgs can be used to infer presence-absence matrices, where rows represent node IDs and columns represent input samples. 
While plain text matrices may not provide the same visual representation of genome variations as graphs do, they offer distinct advantages:
\begin{enumerate}
	\item They are a more accessible starting point for downstream applications that \dbgs;
	\item bio-statistical methods and population genetics can be easily applied to this model;
	\item provide a standardized format that can be compatible with various computational tools and libraries, providing a more reliable framework for researchers to build upon. This is in distinct contrast to color representation for \ccdbgs that depend on the particular implementation.
\end{enumerate} 
The significance of \muset lies in its novel approach to generate plain text unitig matrices. Prior to the development of the \muset pipeline, abundance matrices could be theoretically generated only from paths in variation graphs. Our tool makes now possible to obtain a sufficiently precise and compact representation that enables new kind of analysis in pangenomics, transcriptomics, paleogenomics and metagenomics.\\
\subsection{Improving the method}
\muset is the first pipeline that integrates various existing and novel tools to produce unitig matrix representations of pangenomes. The \kmat software has been specifically developed for the pipeline: it implements filtering, count averaging, presence-absence steps and handles the various input/outputs. Most of the tools incorporated in \muset were instead not originally designed for this particular application.\\
While \muset's main contribution is in proposing for the first time a method to build such matrices, the current implementation has room for improvement and optimization.
The pipeline involves multiple steps in which data is written and read to disk, not for a specific design choice to ease the burden from in-memory processing, but because of subsequent steps are done by different software. Other operations are also slowing down the pipeline. Two of these that are easy to spot are: building unitigs from a file containing one \kmer per sequence in the abundance pipeline and transforming the jsonl output of \ggcat to a csv instead of using \ggcat APIs to get presence values to avoid two I/O operations and dumping directly to disk the csv matrix in the presence-absence implementation.\\
Addressing these bottlenecks could yield a more optimized method to generate abundance unitig matrices that could scale to larger datasets and allow matrix-based analysis on them.
%Another, more simple, upgrade of muset would be to produce such matrices also from a variation graph to allow 

\section{Prototyping Dynamic Data structures for \kmer counting: a Rank Select Quotient Filter}
\label{sec:qf}
While abundance unitig matrices are valuable for specific genomics applications, they may not be the most efficient representation for all use-cases. In particular, some applications require rapid determination of presence or absence of specific sequences in datasets of interest. By indexing the dataset, the information is organized in data structures that enable fast data queries.\\
Sequence query in \kmer based data structures is typically performed through pseudo-alignment, a technique that involves tokenizing sequences into \kmers and querying each of them. Some data structures exploit the difference of just 1 character between subsequent \kmers to query to rapidly query multiple of them. While \cdbg or \ccdbg can be used to address this task, the graph construction is a major bottleneck and requires explicit association between each \kmer and its abundance. \\
\subsection{Brief filter data structures overview}
\kmer storing data structures imply mainly two strategies to efficiently store and retrieve them: representing \kmers as string or as hashes\cite{marchet2024kmersets}. In this work we will describe a method that stores \kmers as hashes and in particular uses structures with false positive rates. Other hash-based data structures are static or dynamic hash tables~\cite{sshash}.\\
Recall from~\ref{sec:introduction_qf} that approximate Membership Query (AMQ) data structures, such as Bloom filters, quotient filters, and cuckoo filters, offer a more space-efficient representation of a set or multi set of elements. These data structures have become essential tools not only in computational biology but also in other domains like databases, storage systems and networks. They are termed approximate because they allow queries to return a controlled false positive value at a rate $\delta$. This means that while they always confirm the presence of an inserted item, they might erroneously return true for non-inserted items with a probability of $\delta$. This trade-off provides space savings.\\
Recent developments in the \kmer field yielded improved version of data structures that allow to store the count of elements in a dataset, instead of just reporting the absence or presence~\cite{squeakr,pandey,cqf,comin_count,sshash}. AMQs that provide this feature are termed counting AMQ or \gls{cAMQ}. In the case of cAMQ, false positive values should report a count greater and not inferior to the real ones: this means that they should never underestimate the abundance of an element in the dataset. These data structures are particularly useful in many bioinformatics applications, from pangenomics to transcriptomics and metagenomics.\\
At the present moment, the main areas of improvement of such data structures are:
\begin{enumerate}
	\item latency of \memb operations. It determines the range of applications they can be used for (e.g., computer networks) and the volume of data that can be queried within a reasonable amount of time (e.g., bioinformatics). The latency is mainly influenced by:
	\begin{itemize}
		\item data locality design. Developing an architecture that reduces cache misses by storing data that might be frequently accessed together in slots that fit within the largest process cache.
		\item implementation efficiency, by carefully engineering the software for code branches, SIMD operations and multiprocessing. This is aspect is particularly important, as theoretically less efficient data structures can outperform others in practice due to optimization.
	\end{itemize}
	\item space efficiency, as it poses constraints on the computer architectures that can utilize it. Despite RAM cost declining, the more rapid growth of data requires efficient and parsimonious encoding of data to the bit level.
	\item supported features. The possibility to modify the data contained after initialization. The most useful operations are:
	\begin{itemize}
		\item incrementing elements count, or add new ones; 
		\item decrementing elements count, or removing them when their count reaches zero;
		\item enumerating elements with their respective count;
		\item automatically resize the filter when it reaches maximum capacity, avoiding the need for a priori cardinality estimation and allowing for addition of new elements.
	\end{itemize}
\end{enumerate}
A data structure that addresses efficiently all these aspects is the Counting Quotient Filter or \gls{CQF}. It is based on the Rank-Select quotient filter and it uses a counting scheme to efficiently encode the count of inserted elements in the slots of the filter. This method offers reduced memory usage and faster lookup speed compared to inserting multiple times elements in a Quotient Filter to remember the counts.
\subsubsection{Filter data structures}
AMQs are often termed with the name of filter. Here I briefly describe the most common ones.
\begin{itemize}[leftmargin=1.8cm]
	\item[The \textbf{Bloom filter}] is a well-known \gls{AMQ}, which uses hash functions to map inserted items into a bit vector. Despite its space efficiency (one to two bytes per element for common $\delta$ values like 1/50 to 1/1000), a major drawback is it cannot be resized and doesn't support deletions. Counting Bloom filters (\gls{CBF}) extend Bloom filters by using saturating counters instead of bits, enabling deletions at the cost of increased space. Scalable Bloom filters, on the other hand, maintain a low false-positive rate even when the number of items is unknown by employing multiple Bloom filters.
	\item[The \textbf{Quotient filter}] uses hashed fingerprints to manage table slots. It supports a range of operations like insertion, deletion, and resizing. It is more cache-efficient and faster than Bloom filters—though less space-efficient than CBFs—making it suitable for systems like \gls{SSD}s. One downside is that its performance degrades significantly once the table exceeds 60\% occupancy.
	\item[The \textbf{Cuckoo filter}] uses cuckoo hashing. It uses two potential slots for storing each item and moves items between their alternate locations if needed, causing a cascade of movements (kicks) until a stable arrangement is achieved. This filter is fast for lookups but can suffer from poor cache performance if many kicks occur, especially when the structure becomes full.
\end{itemize}
All these implementations provide a fast lookup table where information can be stored in fixed-size slots.\\
I will focus on the implementation of the Quotient Filter, as it forms the basis of our implementation.

\subsection{Quotient Filter structure: Rank and Select}
The Rank-Select quotient filter, or \gls{RSQF}, works by hashing items into a $p$-bit fingerprint $x$ and then dividing the bits into two parts: the quotient $h0(x)$ of $q$ bits and the remainder $h1(x)$ of $r = p - q$ bits. The RSQF has an array of $2^q$ $r$-bit slots that store the remainder of each item. When inserting an item, the process begins by attempting to place the remainder in the slot determined by the quotient. If the home slot is occupied, a linear probing technique is used to locate the next available slot.
To track and manage the runs (collections of subsequent slots containing remainders of a specific quotient), the the RSQF uses two metadata bitvectors:
\begin{itemize}
	\item[\occs] indicates for which quotients there is already a remainder stored in the filter slots;
	\item[\rends] indicates the end of each set of consecutive entries associated to the same quotient (also denoted as "run") in the filter.
\end{itemize}
The combination of these two bit vectors allows the RSQF to efficiently locate and manage inserted items by using rank and select operations. Specifically, the \texttt{RANK} function counts the number of \occ slots up to a certain point, and  \texttt{SELECT} identifies where in the filter a particular run ends. This allows efficient lookup, insertion, and enumeration of the data in the filter.\\
Moreover, to store the data in a cache-friendly way, the filter is structured into blocks of 64 slots. To minimize operations requiring the scan of multiple blocks when the filter is relatively full, an offsets array tracks the distance from the start of a run to where it ends, every 64 slot. Computing offsets involves scanning only small sections of the metadata (64 bits or fewer) per operation, making the filter significantly faster.\\
Each block fragments contains one offset value, 64 \occs bits, 64 \rends bits, and 64 slots for remainders data. By grouping these elements together, the system minimizes memory access and enhances cache efficiency. This structure is optimized for rapid traversal and manipulation of the filter content to enable fast membership queries and updates.\\
A scheme of how the quotient filter works is shown in figure~\ref{fig:qf_ex}. For the sake of simplicity the metadata vectors are not shown.
\begin{figure}[h!]
	\centering
	\includegraphics[width=.95\linewidth]{figures/kmer_methods/quotient_filter_export.pdf}
	\caption[Example of a quotient filter.]{Example of a quotient filter. An integer, represented in decimal form, can be expressed as a binary sequence. An unsigned 16-bit integer is composed of 16 binary digits (bits) that can store a value from $0$ to $2^{16}-1 = 65535$. To construct a quotient filter, a hash value is generated by applying some binary operations on this integer. From the computed hash value, the leftmost 4 bits are considered the quotient (although, depending on the implementation, the rightmost bits could be used instead). The remaining bits from the hash form the remainder. The remainder is stored in the filter at the position indicated by the quotient. }
	\label{fig:qf_ex}
\end{figure}
\subsubsection*{Counting}
\label{sec:rsqfcount}
Counting data structures can store either exact counts or order of magnitude, depending on the application. In fields like human genomics, where count numbers are expected to be relatively low and precision is crucial, exact counts are preferred. Differently, in fields like metagenomics, where skewed abundance is more probable, an estimate of the count is often sufficient.\\
Counts can be stored using three different strategies. In all cases, the remainders associated with a certain quotient are stored in monotonic order within the data structure.
\begin{enumerate}
	\item  a count can be encoded as the number of times a remainder is inserted into consecutive slots. This is the most basic implementation and in most cases the least space-efficient one. The data structure occupation is directly related to the total number of counts in it, making poor use of the bits in the slots.
	\item a count $N$ of a remainder $R$ can be encoded into multiple consecutive slots as follows: 
	\begin{itemize}
		\item if $1 <= N <= 2$, than the remainder $R$ is inserted $N$ times;
		\item else $K = N - 2$ can be encoded into a sequence of slots. The boundaries are flagged by two remainders $R$. The slots in between store the count $K$. To signal that the slots are used as counter, the count elements break the monotonicity just after the first remainder.
	\end{itemize}
	This approach uses at least 3 slots for $n>2$ and, while more efficient than the first, is still inefficient for low count values.
	\item another way of storing the counter value is reserving $m$ extra bits for every slot to encode in it the count associated to the remainder. As this method adds $2^q * c$ bits the choice of $c$ should be calibrated for the suited application. 
\end{enumerate}

\subsection{Developing a new library for a Quotient Filter}
The existing implementation of the Counting Quotient Filter (CQF), as described in \cite{cqf} and available at \texttt{https://github.com/splatlab/cqf}, presents several limitations that restrict its versatility and applicability. First, the generated quotient filter is constructed as a static object, meaning that once created, its structure and content cannot be modified. This constrains the ability to dynamically adjust or update the filter after its initial creation, which is often the case in many applications.
Second, the implementation is specifically focused on the CQF, which limits the potential for experimenting with and developing variations based on the RSQF or the CQF itself that can add new functionalities or improve performances in specific cases. Furthermore, the existing implementation does not explicitly address the concept of "toricity" in the filter. With the term toricity, I refer to the circular logic of the filter, where elements can be moved around from the end to the beginning of the data structure, if needed. This property is crucial to fill properly handle the insertion of hashes with highest quotient value and when the filter is near capacity.\\
To address these limitations,we have re-implemented the data structure with these features:
\begin{itemize}
	\item it can be exact (when no space trade-off is chosen) or approximate in the \kmer space;
	\item it handles exact or approximate counts;
	\item it handles different count encoding;
\end{itemize}
The data structure is layered as follows:
\begin{itemize}[leftmargin=1.8cm]
	\item[\textbf{Low Level}] comprises agnostic operations in the data structure such as
	\begin{itemize}
		\item setting a specific slot to a certain value. The value can be a remainder, count or combination of the two. It can also be any metadata as it only imposes bits to a certain region of memory corresponding to a slot;
		\item clearing of a slot to zeros for removals of elements from the filter and shifting operations;
		\item reading the value from specific slots;
		\item shifting slots by a certain amount $z$. Slots starting at position $x$ to $y$ are moved between positions $x+z$ and $y+z$ modulo the length of the filter. This operation is needed when a new element has to be inserted in an already occupied position. By shifting right of $z$ slots the already set slots, the position $x$ to $x+z-1$ are therefore free to accommodate new elements or counts;
		\item metadata operations for setting to 1 or to 0 the \occs and \rends bits in their bitvectors;
		\item updating the offset vector to track of runs.
	\end{itemize}
	\item[\textbf{Medium Level}] it comprises operation to insert, remove and query elements without explicitly handling bit-level operations. They implement a RSQF data structure.
	\begin{itemize}
		\item addition of an element to the data structure;
		\item removal of an element from the data structure;
		\item query of an element from the data structure;
		\item metadata handling in each of the aforementioned operations.
	\end{itemize}
	\item[\textbf{High Level}] it comprises operations built on top of the RSQF layer and described a CQF.
	\begin{itemize}
		\item extended addition and removal to allow for multiple slots altogether;
		\item functions to encode and decode counters, as described in point 2 of section~\ref{sec:rsqfcount};
		\item query operations using specific linear probe, recognizing the start and end of a counter.
	\end{itemize}
	\item[\textbf{Application Specific}] functions specific for handling input, output and resize.
	\begin{itemize}
		\item \kmer hashing.
		\item initialization of the data structure;
		\item enumeration of elements within the data structure;
		\item dynamic resizing of the data structure, including element enumeration, filter size doubling, and element reinsertion;
	\end{itemize} 
\end{itemize}
This implementation has been developed as both a library and standalone software, offering flexibility in its application. It is available at \texttt{https://github.com/frankandreace/cqf\_implementation/}.
\subsubsection{Allowing multiple types of counts: CQF and BQF}
While the primary focus was on re-implementing a Counting Quotient Filter (CQF) from the Rank-Select Quotient Filter (RSQF) implementation, the basic structure with low to mid-level operations and application operations can be used to build other models on top of the RSQF. Victor Levallois has successfully implemented another data structure called the Backpack Quotient Filter (\gls{BQF})~\cite{bqf}, which uses the count encoding presented in point 3 of the previous section on RSQF counting methods~\ref{sec:rsqfcount}. Instead of using other slots to encode the counter, it reserves a pre-defined amount of bits in the remainder to store the approximate count:this is done at the expense of some precision.\\
The successful implementation of the BQF demonstrates the flexibility and extensibility of the proposed architecture in order to handle different kind of counts.

\subsubsection{Handling toricity}
A crucial property of the filter, overlooked in the original CQF implementation, is the handling of specific edge cases related to its toroidal structure. Runs of remainders (or counts or combinations of both) associated with the same quotient are stored contiguously in monotonic order. This storage must be done in increasing slot ID order. When two or more elements with the same quotient are added to an empty filter, the slots used are the ones associated with the quotient and \emph{on the right of it} (i.e., the slots associated with the next quotients), and so on.\\
A challenge arises in the part of the filter having the highest quotient values. When new elements are added, it's likely that at some point, a slot should be pushed to the next element after the final slot. To address this issue, the filter is implemented with a toroidal structure, meaning the slot following the last slot is the first slot of the filter. This property eliminates problems associated with filling the filter in the final slots by imposing equal properties to all slots in the filter.\\
Implementing the filter with this characteristic is non-trivial, as the toroidal property requires different handling of all comparisons within functions and shifting operations. It necessitates careful consideration of edge cases and boundary conditions to ensure correct behavior when operations wrap around from the end to the beginning of the filter.\\
This toroidal implementation enhances the robustness and efficiency of the filter, allowing for more uniform utilization of the entire data structure and eliminating potential bottlenecks that could occur at the boundaries of a linear structure. Figure~\ref{fig:cqf_toricity} shows how the toroidal property works in a case of insertion where the final part of the remainder vector is already full.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\linewidth]{figures/kmer_methods/cqf_toricity_export.pdf}
	\caption[The RSQF data structure scheme and toroidal property]{a) Scheme of the RSQF data structure partitioned into 16 elements blocks. Along with the remainders vector, each block consists of the correspondent block of \occs and \rends bitvectors and one offset value. When elements associated to a certain quotient (here $16*x + 7$) cannot be stored in the relative slot because it is already used by another run, they are placed into the next contiguous available slots.\\
	b) example of toroidal insertion. One remainder associated to quotient $16*N + 14$ is inserted in the already partially filled data structure. Since there are no free slots in the final part of the remainder vector, the next available free slot is at the beginning of the vector. The first slot of the vector, which was free, is then filled with the remainder. Metadata bits are then updated. The \occ bit of the quotient $16*N + 14$ is set to 1. The \rend at position 0 is then set to 1 and this updates the offset.} 
	\label{fig:cqf_toricity}
\end{figure}

\subsubsection{Dynamic resizing strategy}
The proposed implementation of the filter eliminates the need for a priori computation of the \kmer set cardinality (and estimating the slots used by the counts), allowing for direct data insertion. To prevent failure when the number of inserted elements exceeds capacity, a dynamic resize and reallocation strategy is implemented. A specific counter is incremented each time a slot is used in the filter. This counter keeps track of the slot occupancy ratio: when the number of occupied slots reaches 95\%, the insertion process is paused and the resize operation is initiated. The process operates as follows: 
\begin{enumerate}
	\item Enumeration: all stored hashes (with their counts) are enumerated. Hash values are temporarily stored in a hash table, with the element as the key and the count as the value. The enumeration is done by linear probing the entire filter. Each \occ metadata bit is sequentially scanned. When a bit is set to one, the probe looks for the corresponding run of remainders in the filters and proceeds to enumerating all the elements (composed of the combination of quotient and remainder) with their counts. When the entire \occs bitvector is scanned, the enumeration ends.
	\item Parameters update: the size of the quotient is increased of one and the size of the remainder is decreased of the same amount. By adding 1 bit to the quotient, the number of index-able and insert-able elements doubles. All other internal values depending on quotient or remainder are updated. The occupancy counter is set to 0.
	\item Re-instantiation of the vectors: the remainder vector and the metadata vectors are doubled in size.
	\item Insertion of elements in the new filter. The elements are fetched from the hash table and inserted back in the filter.
\end{enumerate}
The dynamic resizing allows for flexible use of the data structure without prior knowledge of data size and ensures efficient space utilization by growing only when necessary. However, this process produces a temporary spike in memory, by having at the same time the map containing the enumeration and the newly doubled filter.

\subsubsection{Using the Fimpera scheme to reduce space}
The size of a RSQF data structure is given by $2^q * (r + m )$ bits, where $m\sim 2.125$ metadata bits (along with some other relatively negligible overhead). This suggests that if there were a way to store nearly the same input information while reducing $r$, it would yield substantial space savings and enable the use of the data structure on larger datasets. To achieve this goal, the CQF implementation has been developed also with the Fimpera scheme incorporated~\cite{fimpera}, like in the BQF.

Fimpera operates by splitting each \kmer into smaller \emph{s-mers} and storing them in the filter, each with the \kmer count. The \kmer abundance can thus be retrieved through its constituent \emph{s-mers}, as the presence of a\kmer implies the presence of all its constituent \emph{s-mers}.\\
This approach has been demonstrated to significantly reduce the false-positive rate by an order of magnitude without generating false negatives or underestimating \kmer abundance~\cite{fimpera}. In this context, Fimpera is used to reduce the dimension of the filter without increasing the false-positive rate, as storing smaller hashes from \emph{s-mers} results in a smaller $r$.

To estimate the correct abundance of a \kmer, the smallest count among its constituent \emph{s-mers} is used.
The major drawback of this scheme is the loss of the \kmer enumeration feature, as only the \emph{s-mers} can be retrieved. However, the dynamic resizing of the data structure is preserved as only \emph{s-mers} are required to be remembered.

This approach may introduce false positive \kmers: \kmers that are non present in the original dataset but are composed of \emph{s-mers} found in other \kmers are going to be reported as present. As this is a joint probability, it is the produce of each independent probability, becoming very low when $s$ is sufficiently large. The $s$ parameter must be chosen as a trade-off between space efficiency and false positive rates. For the BQF, this has been estimated as under $10^{-5}\%$ with $s > 20$ when $k = 32$.

This Fimpera-enhanced implementation offers a novel approach to improving the space efficiency of RSQF-based data structures while maintaining acceptable error rates and proves the flexibility of the proposed implementation.

\subsection{Conclusions and perspectives}
The proposed implementation of a prototype quotient filter has demonstrated considerable flexibility in its approach to storing \kmer abundances. This adaptability is a key feature that sets it apart from more rigid data structures and opens up new possibilities for genomic data analysis.\\
A possible extension of this project would be to incorporate additional metadata, which would extend its functionality beyond a conventional cAMQ. For instance, color information would be greatly useful for specific pangenomics applications. However, this extension is non-trivial due to the memory-intensive nature of color storage and encoding. An interesting direction could use of Shannon coding for colors. When the filter undergoes multiple resizes, each resize operation can be used to evaluate the distribution of contained colors. The result of this evaluation could be used to inform an adaptive encoding strategy where frequently occurring colors are represented with fewer bits, while less common colors are encoded with into large values. Such an approach would optimize the space-color information trade-off dynamically.\\
Despite its new features, this implementation has not found its way to a standalone publication. This proposed CQF implementation (without Fimpera) exhibited slower performance compared to the original, more optimized, version. This performance disparity highlights the challenges providing implementations with new features without impacting computational efficiency. The negative results in terms of insertion and query speed on initial testing of the tool together with the positive results of the BQF implementation were the reasons we decided to not move forward with more extensive benchmarking of the tool. Code profiling showed bottlenecks related to the insertion of counts and the encoding and the coding of counts. Based on the same RSQF, the BQF uses less operations as it modifies only one slot at a time, compared to the multiple slots of the CQF.\\
Nevertheless, there is value in prototyping data structures that offer greater customization potential for specific applications, as they provide guidance for the evolution of more flexible and adaptable data structures.\\
Finally, the BQF implementation, based on this RSQF model, has been presented at RECOMBseq~\cite{recombseq} conference and will be soon published in the associated journal~\cite{bqf}.

\section{Prototyping Dynamic Data structures for \kmer counting: \skmer sorted list}
\label{sec:skmers}
In this chapter, I have examined various examples of \kmer-based methods, demonstrating that there is no universal approach to efficiently represent \kmers. The most appropriate method is largely dependent on the specific application requirements. For instance, when comparing two datasets, one effective approach is to enumerate all \kmers for each set and store them in sorted lists. The difference between these sets can then be quantified using a set metric, such as the Jaccard index, which accounts for \kmer differences in the lists. This process is particularly efficient when working with sorted lists. sorted \kmer lists offer a reasonably efficient representation for individual \kmer queries without the need for additional indexing structures. Using binary search, the query time complexity is $O(log{N})$, where $N$ represents the cardinality of the set. This logarithmic time complexity ensures relatively fast query operations, especially for moderately sized sets.\\
While not indexing the data results in slower data structure insertion time, using sorted lists offers the advantage of reduced space requirements. A map index achieves $O(1)$ time for each insertion, leading to $O(N)$ complexity when inserting $N$ elements. In contrast, sorting a list, assuming random data without additional hypotheses, requires $O(Nlog{N})$ time when comparisons are performed. Utilizing a list instead of a more complex data structure, such as the RSQF, provides a more compact representation of the data in memory. The list has no overhead as it does not use empty slots, metadata (\occs, \rends or offsets) vectors, or additional variables. The size of the data structure is solely determined by the size of the raw data; no external bits of information are added, even though information is gained compared to the input through the sorting process.\\
Instead of storing individual \kmers, an encoding can be employed to group overlapping \kmer together, resulting in space savings. The \kmer research community has explored several space compression techniques by encoding \kmers within string sets. The fundamental idea is to construct string sets that contain all enumerated \kmers and nothing else. Recent models include unitigs, eulertigs, simplitigs, and \skmers ~\cite{marchet2024kmersets}.\\
We propose a novel data structure that combines the encoding of \kmers into \skmers for space savings with a sorted list structure for relatively quick non-indexed \kmer queries. This structure can be termed a \skmers sorted list.
\skmers are sequences of adjacent \kmers that share the same minimizer. They generate a partition of the \kmer set, allowing for space savings and parallelization. Although they are a string representation, \skmers are often used in hash-based methods as they are based on hashed minimizers. Another advantage of using \skmers in the sorted list is that consecutive \kmers, being close to each other, can be queried together, resulting in significant computational time savings. To perform a query, the algorithm locates the \skmer linked to the queried \kmer's minimizer and checks for the \kmer within that \skmer.\\
Implementations like \blight~\cite{blight} and \ssh~\cite{sshash} use Minimal Perfect Hash Functions (\gls{MPHF}) to facilitate mapping of minimizers to their corresponding \skmers. However, the use of MPHF generates a static dictionary that cannot be updated~\cite{smsketch}, forcing the data structure to remain static. By employing sorted \skmers lists instead, we can provide dynamic updates, enhancing the flexibility and adaptability of the data structure.

\subsection{Super-\kmers sorted lists: Input and output}
The \skmer sorting algorithm we propose is a component of a more comprehensive \skmer-based data structure to which I contributed. For the sake of brevity, we will not delve into the full details of this structure.\\
Our algorithm takes any source of \kmers and processes it to produce an enumeration of \skmer that are then sorted into a list for efficient lookup.\\
While this method can enumerate the sorted \skmer in text format, all the operations are done in binary space after 2-bits encoding the nucleotides. This encoding approach offers dual benefits: it enhances space efficiency for\kmer representation in memory and facilitates fast machine operations for data comparison and modification.\\
The following sections provide a detailed overview of the sorting steps, beginning with the enumeration of \skmers from the input data.

\subsection{Super\kmer list model}
\label{sec:skmer_model}
To better comprehend the sorting algorithm's steps, it is useful to conceptualize the \skmer list as both a succession of string-representing objects and a matrix. This matrix is defined by two key parameters: $N$, the number of enumerated super\kmers; $M$, the maximum number of \kmers that a \skmer can handle, where $M = k-m+1$ ($k$ = \kmer length, $m$ = minimizer length). In this matrix model, rows represent distinct strings (\skmers), while columns identify specific positions within each \skmer. An important aspect of this representation is that a horizontal position in the matrix (column) corresponds to the position of a \kmer within its \skmer. Position 0 represents the first \kmer of the \skmer. This position also corresponds to the suffix size of the \kmer, where the suffix is defined as the number of nucleotides following the minimizer.\\
Most algorithm steps can be conceptualized as operations on the matrix columns. A crucial feature of this model is that a specific position in the matrix represents the presence or absence of a \kmer in the data structure.\\
The matrix-based concept is going to be used in the next sections to provide a clearer and simplified representation of the \skmer list structure to understand algorithmic operations.

%Figure YYY shows the equivalences between these two models.\\
\subsection{The $log{N}$ query model}
Unlike a traditional \kmer list, the \kmers within a \skmer list are not fully ordered. Instead, we have $M$ ordered columns, which allows us to transform the classic binary search algorithm as follows:
\begin{enumerate}
	\item Identify the minimizer position $p$ within the \kmer to be queried.
	\item Navigate to column $p$ of the matrix, focusing only on the \kmers present in that specific column.
	\item Perform a binary search for the target \kmer within the selected column.
\end{enumerate}
Step 1, while potentially requiring up to $k-m+1$ operations, becomes more efficient when searching for successive \kmers as the cost is amortized over multiple queries.
Step 3 maintains the logarithmic time complexity of a binary search, $O(log N)$, where $N$ is the number of \skmers. However, in practice, this step tends to be faster than a traditional binary search on a full \kmer list. This improved efficiency is due to the distribution of \kmers among all columns, effectively reducing the search space for each query.

\subsection{Sorting \kmers from the same matrix column}
\label{sec:skmersorting}
The sorting algorithm's initial step involves a column-wise bucketing of the input \kmer source, based on the minimizer position within each \kmer. This process distributes \kmers across $M$ columns, with $M$ defined as in section~\ref{sec:skmer_model}. Then, each column is independently sorted. Once all columns have been individually sorted, the resulting column-sorted matrix is returned as the final output of this phase. Note that the sorting processes are completely independent and can be performed in parallel.\\

\subsection{Construction of overlap lists between consecutive columns}
\label{sec:skmeroverlap}
The next step in the algorithm focuses on detecting overlaps between consecutive columns. For each pair of adjacent columns, we aim to identify where the $k-1$ suffix of a \kmer in the first column overlaps with the $k-1$ prefix of a \kmer in the subsequent column. This process is performed independently for each pair of consecutive columns and can be divided into several steps. First, the algorithm inserts the $k-1$ prefixes of the \kmers from the second column into a hash table, using the prefix as the key and the \skmer ID as the value. Then for each $k-1$ suffix computed from the first column, it searches for a matching entry in the hash table. When a match is found, the pair of \skmer IDs (one from each column) is inserted into a list of candidate overlaps. It's crucial to note that a single element in one column may overlap with multiple elements in the adjacent column. In such cases, all possible overlaps are recorded.\\
Once the scan is completed, each pair of adjacent columns will have its own list of candidate overlaps. This step is essential for identifying potential connections between \kmers across different \skmers, which will be utilized in subsequent stages of the algorithm. The independent logic of these operations allows for effective parallelization, potentially improving computational efficiency.

\subsection{Maximal set of overlapping \kmers: co-linear chaining}
The previous step computed all possible \kmer overlap pairs. However, to produce a properly sorted list, we must respect the ordering of\kmers across all columns. Some overlaps may be incompatible due to two possible scenarios:
\begin{itemize}
	\item[a] a \kmer overlaps with multiple \kmers in the adjacent column, but can be used just once;
	\item[b] Pairs that, when visualized as edges between column positions, cross each other cannot be used together as they would violate the relative column order.
\end{itemize}
Figure YYY illustrates examples of invalid pairs and a maximal chain. To select the maximal set of "non-crossing" pairs from those calculated earlier, we employ co-linear chaining.\\
Co-linear chaining is an algorithmic technique commonly used in alignment algorithms. In the context of pairing shared subsequences between two sequences, it can be summarized as finding pairs of connected elements such that no "crossing" connections occur when visualized geometrically. \\
In read-to-reference sequence alignment, co-linear chaining takes pairs of maximal exact matches (MEMs) as input. It then computes a chain of pairs where the selected pairs' order aligns with their appearance in both strings, while maximizing the number of bases covered by the chain in the read. Recently, this technique has also been applied to sequence-to-graph alignment in pangenomics applications.\\
In the context of this method, co-linear chaining is used to resolve the compatibility issues between \kmer overlap pairs, ensuring a consistent order across columns while maximizing the number of valid overlaps. This step is crucial to produce a coherent and efficiently searchable structure.

\begin{definition}[Co-linear chaining of \kmers in the matrix]
	Given 
	\begin{itemize}
		\item two ordered lists of super\kmer ids of two contiguous columns computed as in section~\ref{sec:skmersorting}.
		\begin{itemize}
			\item $ A = \{a_1, a_2, ..., a_n\}, |A| = N $, representing the left column,
			\item $ B = \{b_1, b_2, ..., b_m\}, |B| = M $, representing the right column;
		\end{itemize}
		\item a set of tuples $ V = \{(v_1,w_1),(v_2,w_2), ..., (v_k,w_k) \}$ such as $ v_i \in A$ and $ w_i \in B$ and $(v_i,w_i)$ represents an overlap between \kmers of the two contiguous columns, as computed in section~\ref{sec:skmeroverlap};
	\end{itemize}
	The goal is to find the maximal list of tuples  $U$, with $set(U) \subseteq V$ such that 
	\begin{itemize}
		\item if $v_i \prec v_j$ in $U \land v_i=a_i, v_j=a_j \implies a_i \prec a_j$, and $b_i$ < $b_j$;
		\item $\forall v, w \in U | v_i = a_x \lor w_i = b_y \implies \nexists v_j != v_i | v_j = a_x \land \nexists w_j != w_i | w_j = b_y$.  
	\end{itemize}
	The first condition implies that the ordering of the column lists $A$, $B$ is respected in $U$, while the second implies that each \skmer ID from $A$ or $B$ can occur only once in the list of tuples $U$.
\end{definition}

The maximal set is computed using dynamic programming~\cite{genome_scale}.
The co-linear chaining process begins by sorting the tuples in $V$ based on their order in $B$. This ensures that no "crossing" occurs in the $w$ ids, as $w_i = b_i: b_i \prec b_j$ will always be processed before $w_j = b_j$. To resolve ties on equal $v$ values, the ordering of $v_i = a_i$ in $A$ is used.\\
Next, we apply dynamic programming over the $A$ ids to identify the largest set of pairs where the $A$ ids form a non-decreasing sequence. The score is stored for each element $v_i$ in the list of pair. A table $C$ of length $M$ is filled, where index $j$ represents the maximum possible score using tuples from $V$ such that $(v,w) has w \in \{b_1,...,b_j\}$. For any tuple, we derive a recurrence based on whether it violates or not the aforementioned conditions. The recurrence is calculate as follows: 
\begin{equation} 
C[j] = \max_{j':w_{j'} \prec w_j} C[j'] + 1
\end{equation}
if $(v_j,w_j)$ does not overlap with $ V_{j'} \subseteq \{\}(v_1,w_1),...,(v_{j-1},w_{j-1})\}$ which is the non-overlapping subset selected in the previous step.\\
To optimize the search for the best solutions $j'$ among those already computed, we use a binary search tree. It contains the best score for each value $w \in A$. As they are maintained sorted based on $A$ ordering, it allows for $O(\log{n})$ complexity when querying for the best score. When new scores are computed, the tree can also be updated in $O(log{n})$ time. From this follows that the co-linear chaining costs $O(n\log{n})$.

\subsection{Reconciliation and final output}
After computing the lists of "non-crossing" overlaps for each pair using the co-linear chaining algorithm, the next step is to reconcile this information to produce the sorted list. To better understand the rationale of the reconciliation, this process is described using the matrix representation, where \kmers in each column are sorted from top to bottom. The reconciliation process can be divided into two main parts:
\begin{enumerate}
	\item Map creation. Using the lists of non-crossing overlaps, each \kmer in the matrix is added to a map as a key, with its value being the ID of the \skmer it will be inserted into. When one overlap list returns the pair (x,y) and the subsequent list returns (y,z), \kmers x, y, and z will be compacted into the same \skmer in the sorted list. Consequently, they are inserted into the map with the same value (e.g., 1). Other \skmers not to be compacted in the same \skmer will have different values.
	\item Producing the sorted list. Each column of the matrix is assigned a pointer that tracks the row containing the next \kmer to be inserted. A \kmer or a \skmer is inserted into the final sorted list by iterating over these pointers. This part of the process is organized as follows:
	\begin{enumerate}
	\item if a \kmer or \skmer has all its elements flagged by a pointer, it is inserted into the list;
	\item ties are resolved by directly comparing the two \skmers and inserting the one with the smallest hash first;
	\item after inserting an element, the pointers on top of its \kmers are moved to the next row;
	\item this process is repeated by going back to a) until all pointers reach the last element of their column.
	\end{enumerate}
\end{enumerate}
By using this approach, we guarantee that the final list is sorted. Part 2 of the process could be done in parallel from the top and the bottom of the matrix to reduce computation time.

\subsection{Searching the list}
The sorted \skmer list produced by the outlined algorithm enables relatively fast search without indexing \kmers, using binary search. Binary search is an efficient algorithm used to locate a target value within a sorted array: it repeatedly divides the search interval in half and compares the middle element to the target.\\
In the case of looking in a sorted array with smaller values on the left and larger values on the right, if the target is smaller than the middle element, it continues the search in the remaining the left half, if it's larger, in the right half. The process continues until the element is found or the search interval is empty.\\
The \kmer query process in our sorted \skmer list operates as follows:
\begin{enumerate}
	\item the minimizer of the \kmers is computed and the \kmer position in a \skmer is determined.
	\item a mask associated to that position is selected;
	\item the range of the searched list is given by $[x,y]$ and set to $[0,N]$, with $N$ being the length of the list;
	\item the binary search algorithm jumps the middle \skmer of the range $[x,y]$;
	\item if the \skmer does not have a \kmer at the searched position, the search moves to on to the next ones until it finds one that does;
	\item the mask is applied to the \skmer;
	\item the resulting binary value is compared to the one of the \kmer. Here one of these 3 situations can occur:
	\begin{itemize}
		\item the masked \skmer value is greater than the \kmer, than $[x,y]$ gets updated to $[x,y] = [(y-x/2),y]$ ;
		\item the masked \skmer value is smaller than the \kmer, than $[x,y]$ gets updated to $[x,y] = [x,(y-x/2)]$ ;
		\item the item is found, return \texttt{FOUND} or \texttt{TRUE};
	\end{itemize}
	\item if $x = y$ return \texttt{NOT FOUND} or \texttt{FALSE}, else go back to point 4;
\end{enumerate}
The advantage of this \skmer representation is that it allows the search algorithm to jump elements (\skmers) and directly query specific positions (offsets) based on the minimizer position of the queried \kmer. This approach avoids linear probing of entire \skmers.\\
Finally, the time complexity may be worse than binary search due to gaps in the elements of the matrix. Not all \skmers are maximal, therefore some queries at specific positions may be impossible due to the absence of a \kmer. An offset is valid if the \skmer contains a \kmer in that position, and invalid otherwise. To address this, when a lookup encounters an invalid offset, it performs a linear probe on previous or subsequent elements in the list until it finds a \skmer with a valid offset.

\subsection{Bonus optimizations}
We propose three potential optimizations for the presented algorithm:
\begin{itemize}
	\item Adding information to guide the binary search when looking in a \skmer at an invalid offset and having to use linear probing to find a a valid one in the neighbors. This can be achieved by filling the bits of an empty offset in a \skmer with "synthetic" nucleotides. These synthetic nucleotides don't serve as genetic information but provide a middle value between the two closest valid offset values, informing the search direction.
	\item The current lexicographic ordering of \skmers is suboptimal due to its bias towards homopolymers and poly-A regions. We propose an improved ordering strategy that gives more weight to central nucleotides, particularly those in or closer to the minimizer. This approach could lead to a more balanced and efficient search structure.
	\item Instead of using binary search, which compares the searched element with the middle of the remaining search space at each step, we could implement interpolation search to potentially accelerate the process~\cite{pla_complexity}. Interpolation search calculates where in the remaining space the element is likely to be, based on the values at the boundaries of the space and the searched value. If the element isn't found at the calculated position, it uses the same splitting strategy as binary search.
\end{itemize}

\subsection{Conclusion and future work}
This project has resulted in a prototype data structure that represents a k-mer set as a sorted list of super-k-mers. The advantages of this representation include:
\begin{itemize}
	\item unlike some hashing data structures such as Bloom filters, our structure enables the enumeration of \kmers within the set;
	\item by compressing \kmers into \skmers, we achieve reduced storage space compared to an ordered list of individual \kmers;
	\item the structure facilitates relatively efficient direct queries without the necessity for indexing;
	\item it allows for the addition of metadata through a separate, complementary data structure.
\end{itemize}
While this work is ongoing, our objective is to produce a paper shortly after the defense as a novel approach to organizing dynamic datasets using \kmers. As the method is not fully completed and tested at the time of the writing of this manuscript, we also lack of benchmarking to show the computational performances of such a method.\\
Furthermore, this prototype could serve as a pre-processing step for storing \kmers in a RSQF. We could use it to produce a sorted and compact \kmer enumeration for direct insertion into the filter.
This approach sheds light on the gap between space-efficient representations of \kmers as string sets and queryable structures, offering a balance of compression and functionality. The potential applications extend storage, as indicated by its possible use in RSQF pre-processing. As we continue to work on this prototype, we aim to use it as valuable building block for more complex applications in large-scale genomic data.

%\subsubsection{Possible improvements}
%As just described, the super\kmer sorted list can be queried using a variation of binary search in which, when the next super\kmer to be searched has not a valid offset, it has to linear probe the elements in the list for one that does. This slows the query operation, especially in cases where the super\kmers with a valid offset are sparse. \\
%A future improvement of this algorithm would be to mitigate this issue, by adding information on non valid offsets to direct the binary search on the right direction without doing a linear probing. This strategy can be implemented by fill the bits of the super\kmer left blank when a \kmer is not stored in an informative way.\\
%Given two super\kmers $S_1$ and $S_2$ in a super\kmer sorted list and an offset $t$. If there are $n$ super\kmers between $S_1$ and $S_2$ that do not have a valid \kmer at that position $t$ while $S_1$ and $S_2$ do. The strategy is to fill the bits not used by \kmers in the super\kmer with "fake" nucleotides that do not serve as genetic information but that help the search by giving information on where to find the queried \kmer. This can be done by filling the empty bits with an average between the value found in $S_1$ and $S_2$ at the same position, starting to fill the bits from the most significant one, for all $n$ super\kmers. Another approach would also take into account the cardinality $n$ of the elements to fill and, instead of filling all the super\kmers with the same averaged value, it would fill the $n$ elements with progressive values from $S_1$ to $S_2$.

\section{\kmer based method exploration: conclusions}
In this chapter, I have presented three distinct yet interconnected projects, each utilizing \kmer data structures to enhance genomic analysis, with a particular focus on features that can best fit pangenomics applications. These efforts also span different levels of the method development stack.\\
The \muset project, whose novel contribution is a pipeline for plain text unitig matrices generation, has had as my main contribution the development of the pipeline and the function of the novel accompanying software, \kmat, that produces the presence-absence matrix using \ggcat. My involvement in this project stems from the conviction that there is a substantial need for more downstream-focused \kmer tools, as they currently lag behind variation graph-based approaches for genomics analyses. Furthermore, I consider the effort to propose standardized, text-based file formats crucial in helping the community build software upon stable foundations.\\
The development of the RSQF-based library and tool, while not revolutionary in concept, represents a significant addition to the bioinformatics community. This prototype offers new features such as enumeration and dynamic updates, provides a clearer description and resolution of issues like toricity, and serves as a reproducible and reusable platform for building other prototypes or tools, as exemplified by the BQF.\\
The \skmer sorting project is a novel contribution, developed in collaboration with my supervisors. This prototype also uses a novel approach to encoding \skmers as an alternative development direction for data structures representing \kmer sets, specifically optimized for particular applications.\\
\muset is an easy recognizable example of my effort in contributing to the development of \kmer based data structures. The RSQF and \skmer sorting are not directly linked specifically to pangenomics, as they can serve more general purposes. Nevertheless they could be used as building blocks of more comprehensive data structures to store, process and analyze pangenomes.\\
By contributing to these different projects, I have gained a deeper understanding of the strengths and challenges of \kmer based methods. I find these experiences invaluable, as they have provided me with a comprehensive view of the field.

\printbibliography