
\chapter{Perspectives and future work}
\label{sec:perspectives}

\section{On human pangenomics: graphs and beyond}
The results of the analysis I conducted during my PhD, presented in chapter \ref{pap:first} serve as basis to understand what are the features, the limitations and the usefulness of the software that is currently used or designed to build pangenome graphs. These are based upon the latest developments in terms of computer science algorithms to provide the best computational performance now possible and represents a huge leap compared to the currently standard software used for genomic analysis.
Here I will present a few considerations and perspectives that stem from this as well as from 2 more years of thoughts and discussions with peers of my doctoral program, my supervisors and other colleagues and experts in the field. \\
As we are possibly at the beginning of a change of paradigm between linear reference sequences and genomics analysis to pangenome references and pangenomics analysis, there are a few things that need to be addressed as soon as possible. 

\textbf{Reproducibility and stability of computation has to be the main focus of the next years for pangenome reference software developing. \\}
In the case of leading general-purpose pangenome reference building tools, like \pggb and \mcactus, that produce variation graphs, it is of upmost importance that the graph generated from a set of sequences is exactly the same when the same data is fed as input. This means that the heuristics used to generate the variation graph are independent of the ordering of the input sequence and do not contain any stochastic process that might alter the structure of the graph. If a tool can produce two variation graph that can spell the same input genomes but that do not have the same internal order, downstream analysis, like read mapping, loci visualization and other application become biased toward the graph, making it less desirable to genomic analysts that rely on the stability of a linear reference. Current liftover and graph-mapping solutions, in my view, can only be a temporary solution if pangenomics is to be adopted and fully accepted in the genomics and genetics field.

\textbf{There should be guarantees or estimates on the overall biological correctness of pangenome graphs. \\} 
While \mcactus omits centromeric variation, \pggb does at the cost of producing more complex graphs. The trade-off is not trivial as gaining on "variation resolution" leads, also, to graphs that are more difficult to interpret, especially as the number of input genomes increases. 
Moreover, De Bruijn Graphs are difficult to untangle and understand already at small case. 
A very useful and interesting future development would be to design a method to evaluate thoroughly the (computational and biological) quality of the pangenomic data structure produced. This tool would be a necessary Quality Control (QC) step in all custom-pangenome based analysis. For human pangenomics, this would be useful for application where a different reference compared to the HPRC precomputed one is needed, for other cases, like bacteria, virus or fungi, where only specific strains and not the entire species is to be considered.

\textbf{De Bruijn Graph methods need a common color file format or interface to push the development of application-specific tools.}
Mathematically clear, computationally efficient and output-stable de Bruijn Graph methods, like the ones that use colored-compacted \dbgs, won't succeed in being real alternatives of alignment based software to perform pangenomic analysis if there won't be a consensus between the main developers on at least a minimum common interface that let users write tools to exploit the information they contain. Standardized file formats \cite{kff} and interfaces for (colored) queries would help other researchers commit into developing tools for \kmer based approaches, independently of the latest tool in the scene. At the current moment, the landscape is quite diverse and new tool are constantly being developed, discouraging, in my view, the needed investment of resources to develop tools for dbg-based downstream analysis. Writing software for genomics application of \kmer based pangenome representations is crucial to make this representation useful to the end users. As the representation of references is better suited to variation graphs, applications of \kmer based tools could provide added value on genomic studies of specific (sub-)populations.

\textbf{Graphs are not the only \kmer based pangenome representation.}
For this specific use-case, other data structure based on \kmers can be used to extract valuable insights. 
As already described, unitig matrices are a new powerful example of \kmer based data structures that can represent the genetic content of a population and its diversity. 
\kmer or unitig matrices as in \kmtricks~\cite{kmtricks} and \muset  can be seen as a pangenome where rows are \kmers/unitigs present in the whole populations and columns are the specific individuals. Then the value in the matrix could be an absence/presence binary value, defining a \emph{de facto} equivalent representation of a colored \dbg. Unitig matrices then contain the same information of a \ccdbg. \\
In my opinion there is great value in being able to demonstrate equivalence between such representation and to develop tools to change of representation such that, depending on the specific application, each user can decide which is of more interest and not be limited by specific tools building specifically formatted data structures. This is another interesting path forward in the field.

\section{Exploring \kmer data structures}
In the second part of this manuscript I presented three contributions I gave to the \kmer data structures field. Here I present some perspectives based on this work.
\textbf{\kmer based downstream applications are reaching}
Indeed much more work is needed to fill the gaps between methods to efficiently represent \kmers and provide tool-sets that enable a wide range of useful downstream analysis. Some relevant efforts in this sense are the ones that aim to provide nucleotide search capabilities for huge collections of data. Ocean Read Atlas (ORA)~\cite{ora}, uses \kmtricks to provide real-time queries of 1,393 marine seawater metagenome samples obtained from the Tara Oceans project. Logan~\cite{logan} and Metagraph~\cite{metagraph}, that use \kmer based methods to represent the entire World genetic diversity, as a snapshot of the ENA or SRA archives.\\
While these efforts provide novel applications into areas that have not been explored before at this scale, tools that tackle areas dominated by alignment based tools, like genomic analysis, are yet to surface up at scale. \muset is a proposal of an alternative approach to genetic variation studies and aims at partially filling this gap and offer alternative ways of analyzing genomes in an unbiased way, potentially enabling discoveries associated to difficult-to-map regions.\\
\textbf{There is still a lot of research space to improve \kmer based methods for specific applications. Especially on metadata}
The two \kmer based prototype implementations proposed in this manuscript showcase other possible variations of data structures that optimize the representation of \kmers for a specific application. As in computational genomics there are multiple different tasks and there is no one-fits-all solution, these methods offer efficient approaches to solve problems related to represent set of \kmers coming from a set of samples and offer fast answers to membership or abundance queries.\\
As pangenomics is now exploding as a field and the importance to discern between samples is fundamental, these data structures should be improved to allow \emph{colored} queries or multiple metadata queries, like the abundance in a specific sample (like as accessing and index of a \muset matrix, but in a fast and programmatic way).