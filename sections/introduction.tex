\chapter{Introduction}
\label{sec:intro}
As sequencing technology becomes increasingly accessible and accurate, the production of high-quality genome assemblies for organisms of the same species is accelerating. The contrast in processing time is stark: while the Human Genome Project took 13 years to produce its results in 2003, hundreds of genomes of similar quality are now being produced within a few years. Large-scale initiatives such as the UK Biobank, which has sequenced the genomes of 100,000 individuals, present new challenges in efficiently analyzing genetic data from relatively similar populations.\\
Traditionally, genomics software has been developed under the assumption that only one or a few high-quality reference sequences of a single species were available for analyzing limited amounts of new data. Consequently, the wealth of newly available public information is not being fully utilized to enhance the quality of current studies. Furthermore, the velocity and heterogeneity of new sequences deposited in public databases like ENA or SRA render current algorithms inadequate for rapidly analyzing population data that can be inferred from them.\\
To address these limitations, a transformative approach known as pangenomics is emerging. Pangenomics aims to reduce the observational bias inherent in current genomic analyses by capturing the entire genetic diversity within a single population, species, or group of similar organisms into a complex representation called a pangenome. This approach necessitates the development of novel computational methods capable of processing thousands of large and complex genomes.\\
As the field of computational pangenomics is relatively new, there is currently no one-size-fits-all solution that satisfies all requirements and enables straightforward downstream analysis for all genomic applications. Different models are being explored to address various challenges in representing, indexing, compressing, and analyzing pangenomic data.\\
One model gaining significant attention is an improvement on the sequence graph, called a variation graph, which models relationships between shared parts and differences in genomes using nodes and edges. Alternatively, an established method for efficiently representing genomic data involves decomposing variable-length sequences, known as reads, into fixed-size tokens called \kmers. The term \kmer derives from the use of an arbitrary value, $k$, which represents the fixed length of these tokens.\\
The \kmer model has demonstrated its effectiveness in numerous bioinformatics techniques, particularly in assembling genomes from reads of a specific species. More recently, \kmers have gained popularity by enabling superior processing of complex data such as ancient DNA or metagenomes from marine samples compared to other methods. Various data structures have been proposed to represent sequencing data as sets of \kmers, each with its own strengths and limitations.\\
As the field of pangenomics continues to evolve, researchers are exploring and refining these different approaches to develop more comprehensive and efficient methods for analyzing the vast amounts of genomic data being generated. The ultimate goal is to create tools and techniques that can fully leverage the wealth of genomic information now available, leading to more accurate and insightful analyses of genetic diversity within and across species.\\
\section{Research Output}
In this dissertation I present the result of my work on analyzing, developing and applying computational methods, mostly \kmer based, for \pangenomics. In the 40 months I spent at the Institut Pasteur under the supervision of Rayan Chikhi and Yoann Dufresne, I have contributed to several projects on \kmer based data structures and pursued my own research direction on pangenomics.\\
This has resulted in several publications, including one as first author and two as second author. At the moment of the writing of this manuscript I also expect to be author of other articles based on the work presented here.\\
In these years I have been involved in oral and poster presentations, mainly at events related to the Marie Sk≈Çodowska-Curie Actions (MSCA) Innovative Training Network (ITN) I have been part of.\\
Finally, by being part of the student association at Pasteur, I have been involved in several outreach events, as the European Researcher Night in 2022 and 2023, to broadly present the bioinformatics and genomics research fields to the general public in Paris.
\section{General Outline}
The manuscript is organized as follows:
\begin{itemize}
	\item[\textbf{Chapter 2}] An introduction to the subject of the thesis and outline of the manuscript.
	\item[\textbf{Chapter 3}] An introduction of the background concepts of Genomics and Pangenomics.
	\item[\textbf{Chapter 4}] This chapter is structured into two main sections. The first section presents my work on human pangenome graph representation, detailing the methodologies employed and the insights gained from this research. This work contributes to the current efforts to create more comprehensive and accurate representations of human genetic diversity. \\The second section addresses the challenges associated with constructing pangenomes that represent cross-chromosome events in yeast. It explores the complexities of capturing genetic variations that span multiple chromosomes, a particularly challenging aspect of pangenome graphs construction.
	\item[\textbf{Chapter 5}] Presentation of three \kmer based methods to which I contributed. The chapter begins with a concise overview of the current state of the art. It is then divided into three separate sections, each providing a detailed discussion of the respective method, including its underlying principles, implementation details, and the specific contributions.
	\item[\textbf{Chapter 6}] Perspectives.
	\item[\textbf{Chapter 7}] Conclusions.
\end{itemize}

%As sequencing is every day more accessible in terms of cost and better in terms of accuracy, increasingly more high quality assemblies are being produced for organisms of the same species. This is also coming at lower processing time: the Human Genome Project took 13 years to produce its result in 2003, while now genomes of similar quality are being produced by the hundreds in a few years time. In addition to this, Project like the UK BioBank, which sequenced the genome of 100 thousands people, pose the challenge of how to efficiently analyze jointly all the genetic data of a relatively similar population.
%Genomics software has been developed under the assumption that only one or few top grade sequences of a single species were available as reference to help analyzing relatively limited amount of new data. This means the new publicly available information is currently not used to improve the quality of present studies. Moreover, the velocity and heterogeneity of new sequences deposited in public databases, like ENA or SRA, makes current algorithms unfit to jointly analyze rapidly the population data that can be inferred from them. 
%For this reason, a new transformative approach is emerging beyond the single reference genome: \pangenomics.
%Its aim is to reduce the observational bias of current genomic analyses by capturing the entire genetic diversity within a single population, species or group of similar ones, into a complex representation called \pangenome.
%To do so, novel computational methods are therefore needed to process up to thousands large and complex genomes. As the research field of computational \pangenomics is relatively new, at the present moment there is no one-fit-all solution that satisfies the requirements and enable straightforward downstream analysis for all genomics applications. In fact, different models are being used to tackle the various problem of representing, indexing, compressing and 
%The model that is currently more researched is an improvement of a sequence graph, called variation graph, in which relationship between shared parts and differences in genomes are modeled by nodes and edges.
%Alternately, one known way of efficiently represent genomic data is to decompose sequences of variable length, known as reads, into tokens of fixed size, called \kmers. This name comes from the use of an arbitrary value, namely \emph{k}, that is the fixed length of such tokens. The \kmer model has proven its effectiveness in many bioinformatics techniques and particularly for assembling genomes from reads of a particular species. More recently they are gaining success by enabling superior processing of complex data such as ancient DNA or metagenomes from marine samples compared to other methods. Many data structures have been proposed to represent sequencing data into sets of \kmers, each of them with its strengths and limitations.
%In this dissertation I present my work on analyzing, developing and applying computational methods, mostly \kmer based, for \pangenomics. 


%publicly available genomic data has already grown past the peta-base scale.
%New tools are being developed to process such extraordinary wealth of information to efficiently store it and to enable practical analysis and exploration of it.
%One of the most promising way of representing it is to decompose the produced sequences of variable length, known as reads, into tokens of fixed size, called \kmers. This name comes from the use of an arbitrary value, namely \emph{k}, that is the fixed length of such tokens. The \kmer model has proven its effectiveness in many bioinformatics techniques and particularly for assembling genomes from reads of a particular species. More recently they are gaining success by enabling superior processing of complex data such as ancient DNA or metagenomes from marine samples compared to other methods. Many data structures have been proposed to represent sequencing data into sets of \kmers, both coming from one or multiple samples, together with useful metadata like occurrence counts or the sample of provenience. Each of them has its strengths and limitations. At the present moment there is still no one-fits-all solution that is able to represent perfect lossless information from the input data while being extremely efficient computationally and primed for ease of use on downstream applications. In this dissertation I present my work on analysing, developing and using \kmer based methods to represent complex datasets containing multiple samples or individuals. These methods are of use in a novel genomics area called pangenomics, that considers multiple sequences together to infer a more comprehensive picture of the genetic diveristy and richness of a species.
%They also help efficient information retrieval for very complex environmental samples, in which the the challenge is to correctly pull apart the collapsed sequencing information to reconstruct and differentiate the genetic information of the wide variety of organisms present in it.