\chapter{Introduction}
\label{sec:intro}

As sequencing is every day more accessible in terms of cost and better in terms of accuracy, increasingly more high quality assemblies are being produced for organisms of the same species. This is also coming at lower processing time: the Human Genome Project took 13 years to produce its result in 2003, while now genomes of similar quality are being produced by the hundreds in a few years time. In addition to this, Project like the UK BioBank, which sequenced the genome of 100 thousands people, pose the challenge of how to efficiently analyze jointly all the genetic data of a relatively similar population.
Genomics software has been developed under the assumption that only one or few top grade sequences of a single species were available as reference to help analyzing relatively limited amount of new data. This means the new publicly available information is currently not used to improve the quality of present studies. Moreover, the velocity and heterogeneity of new sequences deposited in public databases, like ENA or SRA, makes current algorithms unfit to jointly analyze rapidly the population data that can be inferred from them. 
For this reason, a new transformative approach is emerging beyond the single reference genome: \pangenomics.
Its aim is to reduce the observational bias of current genomic analyses by capturing the entire genetic diversity within a single population, species or group of similar ones, into a complex representation called \pangenome.
To do so, novel computational methods are therefore needed to process up to thousands large and complex genomes. As the research field of computational \pangenomics is relatively new, at the present moment there is no one-fit-all solution that satisfies the requirements and enable straightforward downstream analysis for all genomics applications. In fact, different models are being used to tackle the various problem of representing, indexing, compressing and 
The model that is currently more researched is an improvement of a sequence graph, called variation graph, in which relationship between shared parts and differences in genomes are modeled by nodes and edges.
Alternately, one known way of efficiently represent genomic data is to decompose sequences of variable length, known as reads, into tokens of fixed size, called \kmers. This name comes from the use of an arbitrary value, namely \emph{k}, that is the fixed length of such tokens. The \kmer model has proven its effectiveness in many bioinformatics techniques and particularly for assembling genomes from reads of a particular species. More recently they are gaining success by enabling superior processing of complex data such as ancient DNA or metagenomes from marine samples compared to other methods. Many data structures have been proposed to represent sequencing data into sets of \kmers, each of them with its strengths and limitations.
In this dissertation I present my work on analyzing, developing and applying computational methods, mostly \kmer based, for \pangenomics. 


%publicly available genomic data has already grown past the peta-base scale.
%New tools are being developed to process such extraordinary wealth of information to efficiently store it and to enable practical analysis and exploration of it.
%One of the most promising way of representing it is to decompose the produced sequences of variable length, known as reads, into tokens of fixed size, called \kmers. This name comes from the use of an arbitrary value, namely \emph{k}, that is the fixed length of such tokens. The \kmer model has proven its effectiveness in many bioinformatics techniques and particularly for assembling genomes from reads of a particular species. More recently they are gaining success by enabling superior processing of complex data such as ancient DNA or metagenomes from marine samples compared to other methods. Many data structures have been proposed to represent sequencing data into sets of \kmers, both coming from one or multiple samples, together with useful metadata like occurrence counts or the sample of provenience. Each of them has its strengths and limitations. At the present moment there is still no one-fits-all solution that is able to represent perfect lossless information from the input data while being extremely efficient computationally and primed for ease of use on downstream applications. In this dissertation I present my work on analysing, developing and using \kmer based methods to represent complex datasets containing multiple samples or individuals. These methods are of use in a novel genomics area called pangenomics, that considers multiple sequences together to infer a more comprehensive picture of the genetic diveristy and richness of a species.
%They also help efficient information retrieval for very complex environmental samples, in which the the challenge is to correctly pull apart the collapsed sequencing information to reconstruct and differentiate the genetic information of the wide variety of organisms present in it.