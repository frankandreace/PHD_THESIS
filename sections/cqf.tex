\chapter{Data structures to represent complex sequencing data as \kmer sets}
\label{sec:cqf}

\section{Motivation}
This work originates from the need to exploit, in a tractable way, the sequencing information that is stored in large public databases like the Sequence Read Archive (SRA) from the National Institute of Health (NIH) or the European Nucleotide Archive (ENA) from the  European Bioinformatics Institute (EBI). Increasingly more often, when a non-private sample gets sequenced, it gets uploaded to an online public repository: these are, in fact, growing the amount of stored data exponentially. It is thus paramount to develop data structures and software that can represent and explore at least a significant subset of this data for biologically and medically relevant applications. An important examples of such effort is the Serratus project, that enables the discovery and analysis of public viral data in SRA. \cite{serratus}\\
Apart from this, the only possible interrogation that can be done of this datasets is classical database queries of the metadata associated to the samples. It is thus possible to retreive, for example, all the samples that have been tagged as human genomes or that containing the sequencing of RNA (RNA-seq) from some species. What is not possible at the moment, but would be very useful to have, is to to interrogate one or multiple datasets from these repository for specific nucleotide sequences and determine if they are absent or present in the samples (as well as to know their abundance in it, i.e. how many times it is present). 
The abundance of a specific nucleotide sequence is a useful information when, for example, estimating the distribution of bacterias present in a human gut sample: not only it is useful to know if a specific species or strain of bacteria is in it, but also how big is its population compared to the other in the samples. [ADD CITATIONS]\\
This is not a casual example as metagenomes, especially sea-water ones, are complex biological samples that are quite diverse, with lots of different species or strains in it and with different abundance ratios thus the most complex biological data to analyse. 
%Moreover, being able to compare the sequencing information of a new freshly sequenced sample to the many one already published online, like the Tara Oceans project (see link to ENA) is paramount.
The process just described consists of creating an index of the data of interest in such a way that it is possible to interrogate it in a fast and efficient way. In the next sections, I will describe the landscape of such data structures and present my contribution to BQF, a data-structure tailored for these use-cases that tries to fill the gap between current state-of-the-art tools and the immense wealth of data stored publicly available. I will also introduce the latest project I worked on, that is a cpp library for storing biological sequences in a more efficient way.

\section{Introduction}
Contemporary analysis of sequencing data, i.e. the \emph{reads} that are generated from the processing of sequencing a biological sample, is mostly performed by string algorithms. These algorithms transform the raw string data using ad-hoc data structures to enable users (mostly bioinformaticians in biology-oriented research institutes or hospitals) to elucidate a particular question. Software based on these algorithm is, most of the time, just the beginning of more elaborated pipelines. \\
One subset of the many applications of string algorithms is the generation of data structures (i.e. rearrangement of data) that provide efficient storage of the data and that allow very fast interrogations of the absence, presence or (abundance count) of a particular string in it, i.e. checking if it is present in the data and if so, how many times. These processes are known as indexing and querying. The datasets to index in the use cases elucidated in the section above are in the order of tens of TeraBytes of raw data.\\
A simple but very effective analogy of indexing and querying can be done with books and words. \\
Let's say I remember I have a book in my library that has as main character someone called Ricardo and that tells about a story that is also based in Paris. Without any organization of my library I might need, in the worst case, to sequentially read all my books from start to finish to then find the one that I was looking for. This is not convenient at all, especially if I posses a lot of books. In case I maintained an index of in which book I can find any of the words from my whole library, I could quite rapidly find the few ones that contain a character that get called Ricardito. Even more, if I had also an index that associates places with books that have scenes based in them, I could easily triangulate, without even needing to open a single book, that the one I was looking for is \emph{Travesuras de la niÃ±a mala} of the Nobel Price  Mario Vargas Llosa.
This is an example of how, indexing sequencing data, \emph{the words} and their metadata, \emph{the places}, one can rapidly check which samples, \emph{the books}, contain a requested value, without having to look at the raw data.

%Indexing can be thought as keeping a well organized library: if the books are placed all in random places and there is not account of where one can find it based on the title, looking for a particular book is going to be difficult. Indexing is like reorganizing such a library and at the same time keeping a list of titles with their place in it so that if needed, the presence of the absence of a particular book in the library can be quickly verified. \\
\section{Indexing Data Structures}
During my thesis I focused on \kmer based algorithms, i.e. that use strings of length k as basic unit, because of their proven scalability and mathematical simplicity. Their are also the main common theoretical and technical abstraction that is used by the research group.  
Here I will briefly introduce some of the most used data structures and algorithms to store collections \kmers and, in some cases, their counts. The process to index a dataset using \kmers is to represent it as a set of the \kmers that from their sequences, like remembering the set of unique words inside one or multiple books. This representation has to be searchable so then I can interrogate it for the presence of a particular word.  There are three aspects to keep in count when considering such data structures: the computational complexity of building and, most of all, querying; the size of the data structure that stores the information and the kind of interrogations you can do with the data: presence/absence or metadata (like, for example, counts). This leads to the distinction between exact and approximate indexes and by the ones that support membership or metadata queries.\\
The membership query $memb(x)$ returns 
\kmers and other basic computational aspects that are at the basic of such data structure have been covered in the introduction.

\subsection{Minimum set of operations and characteristics of these data structures}
Given a set of sequencing samples, the data structure must be able to add \kmers from each sample to itself with an \emph{insert()} operation. \\
It also has to be able to return the presence of the absence of an element inside it, using a binary \memb operation (0 for no, 1 for yes). The result of the \memb operation can be exact or approximate, i.e. have false positives. If the data structure contains the number of times a \kmer has been seen in the input sample, the \memb operation will return 0 if it has never been seen, and a value $>= 1$ if the value has been seen 1 or multiple times. The count can be exact or represent an order of magnitude of the counts: this is often needed to not saturate the counts as most datasets have skewed \kmer count distribution. \\
The data structure conserves actively or not its internal state after a membership query. For example when looking in a dictionary if an element is present, the CPU will have inside a chunk of memory containing the queried key-value pair and other ones. Other data structures store explicit variables to remember in which place of their internal representation the \memb operation led to. This is important to notice as most of the times, sequences and not \kmers are queried to the data structure, meaning that \memb operations are done sequentially and on \kmers overlapping with each other. Leveraging these properties makes huge differences in the scalability of such data structures.


\subsection{Simple data structures: sorted list and hash table}
Indexing small sets of \kmers is in trivial. There are two well known data structures that can serve this goal.
The most simple data structure used in computer science to maintain an ordered collection of elements to be searched in less than linear time is a sorted list of elements. By ordering the whole enumeration of the set of \kmers in each sample, one can use a binary search to find a requested \kmer in time $O$($k$log$n$), using $O$($kn$) space. This is feasible for very basic cases with small set of \kmers but it is intractable for the aforementioned use-cases, as both time to query a single element or store the dataset scale too poorly.\\
Hash-tables, a well known implementation of dictionaries (or maps) in computer science, solve the problem of the query time, bringing it to $O$($k$) or $O$(1), depending on the particular hash function used. They still require $O(kn)$ space that makes them still unusable for large collections of data. 

\subsection{Approximate membership and filters}
Approximate membership data structure offer a trade-off between the space (in memory or disk) used to store the ingested information and probability of returning a correct answer. While a sorted list or a hash table return always the correct answer to a , these data structures answer with a non-zero probability of false-positive (i.e. reporting a \kmer present in the raw data when in fact it is not) and zero false-negative rates (i.e. reporting a \kmer as not present while it was present). 
\subsubsection{Bloom filters}
\subsubsection{Quotient Filters}

\subsection{De Bruijn Graphs and Unitigs}
De Brujn graphs and unitigs (compacted dbgs) can represent sets of kmers. Colored (compacted) \dbg are data structures that also remember the sample of origin of the \kmers stored in the compacted \dbg. These data structure represent the input sequences as a set of unique \kmers or unitigs. Using \emph{membersgip} operations together to the possibility to navigate the graph, speeds up the queries.
While one can build the adjacency list with and hash table to represent the graph, in order to index the unitigs for membership queries, various approaches have been proposed based on building the FM-Index or other BTW strucutres for the unitigs (DBGFM, deGSM), building an hash table with minimizers as key and untigis as values (Bifrost), MPHF (pufferfish, BLight, sshash).
%To do so, one could  a hash table or, when can be represented in $O(n)$ space (like the BOSS data structure), they need quite convoluted operations and do not provide substancial improvement in query time.

\section{Contribution}
To this end, I collaborated with people from my group and from an external group at INRIA Rennes, to improve the current availability of such tools. The collaboration resulted in the work now outlined: I contributed with writing the reimplementation of a libray of a dynamic Ranks and Select filter. From this the group in Rennes took over to produce a publication on a new dynamic data structure, that has been recently published as the Backpack Quotient Filter, that is built upon the Rank-Select Quotient Filter. My work continued on the development of a Counting Quotient Filter, a data structure per se already concived, and together with the use of a particular scheme, the Fimpera Scheme, that is used also in the CQF.
The result is then the distribution to the community of new tools that provide an improvement to the current state-of-the art in terms of novelty (the BQF, from the group in Rennes) or better availability, reproducibility, or performances (as our version of the CQF and the Fimpera scheme on top of it).

\section{Rank-Select Quotient Filters}
\section{Counting Quotient Filters}
\section{Fimpera}
\section{Conclusions and Perspectives}